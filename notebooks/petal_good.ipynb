{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MIOFlow.losses import MMD_loss, OT_loss, Density_loss, Local_density_loss\n",
    "from MIOFlow.utils import group_extract, sample, to_np, generate_steps\n",
    "from MIOFlow.models import ToyModel, make_model, Autoencoder\n",
    "from MIOFlow.plots import plot_comparision, plot_losses\n",
    "from MIOFlow.train import train, train_ae\n",
    "from MIOFlow.constants import ROOT_DIR, DATA_DIR, NTBK_DIR, IMGS_DIR, RES_DIR\n",
    "from MIOFlow.datasets import (\n",
    "    make_diamonds, make_swiss_roll, make_tree, make_eb_data, \n",
    "    make_dyngen_data, relabel_data\n",
    ")\n",
    "from MIOFlow.ode import NeuralODE, ODEF\n",
    "from MIOFlow.geo import DiffusionDistance, old_DiffusionDistance\n",
    "from MIOFlow.exp import setup_exp\n",
    "from MIOFlow.eval import generate_plot_data\n",
    "\n",
    "import os, pandas as pd, numpy as np, \\\n",
    "    seaborn as sns, matplotlib as mpl, matplotlib.pyplot as plt, \\\n",
    "    torch, torch.nn as nn\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from phate import PHATE\n",
    "\n",
    "# for geodesic learning\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_dims = None\n",
    "round_labels=None\n",
    "use_gaussian=None\n",
    "add_noise_directly=None\n",
    "add_noise_after_phate=None\n",
    "scale_factor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_diamonds()\n",
    "sns.scatterplot(data=df, x='d1', y='d2', hue='samples', palette='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train autoencoder or the geodesic embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if hold one out is True and hold_out not 'random', we train the DAE without this sample\n",
    "groups = sorted(df.samples.unique())\n",
    "hold_one_out = False\n",
    "hold_out = 5\n",
    "\n",
    "if hold_one_out is True and hold_out in groups:\n",
    "    df_ho = df.drop(df[df['samples']==hold_out].index, inplace=False)\n",
    "    groups = sorted(df_ho.samples.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "import time\n",
    "start_time_geo = time.time()\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "model_features = len(df.columns) - 1\n",
    "encoder_layers = [model_features,8,32]\n",
    "\n",
    "dae = Autoencoder(\n",
    "    encoder_layers = encoder_layers,\n",
    "    decoder_layers = encoder_layers[::-1],\n",
    "    activation='ReLU'\n",
    ")\n",
    "optimizer = torch.optim.AdamW(dae.parameters())\n",
    "dae.cuda() if use_cuda else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist=None and recon=True for DAE or dist=DiffusionDistance(knn=40,t_max=3) recon=False for geo embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_DiffusionDistance(RBF(5.0),t_max=3),DiffusionDistance(knn=40,t_max=3,symmetrize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = old_DiffusionDistance(RBF(0.1),t_max=5)\n",
    "n_epochs_emb=1000\n",
    "samples_size_emb = (30,)\n",
    "recon_emb = False\n",
    "if hold_one_out:\n",
    "    losses = train_ae(\n",
    "            dae, df_ho, groups, optimizer, n_epochs=n_epochs_emb, sample_size=samples_size_emb,\n",
    "        noise_min_scale=0.09, noise_max_scale=0.15, dist=dist, recon=recon_emb\n",
    "    )\n",
    "else:\n",
    "    losses = train_ae(\n",
    "        dae, df, groups, optimizer, n_epochs=n_epochs_emb, sample_size=samples_size_emb,\n",
    "    noise_min_scale=0.09, noise_max_scale=0.15, dist=dist, recon=recon_emb\n",
    "    )\n",
    "run_time_geo = time.time() - start_time_geo\n",
    "print(run_time_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if we trained geo and reconstruction at the same time, then we use geo even if 'geo_emb=None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "#exp_name = 'petal_leave{}'.format(hold_out)\n",
    "exp_name='petal_penalty_lowenergy'\n",
    "\n",
    "use_geo = True\n",
    "use_dae = False\n",
    "use_density_loss = True\n",
    "lambda_density = 30\n",
    "top_k=5\n",
    "hinge_value = 0.01\n",
    "use_penalty_energy=True\n",
    "lambda_energy=0.01\n",
    "\n",
    "small_model = True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "emb_features = 5\n",
    "model_features = len(df.columns) - 1 if not use_dae else encoder_layers[-1]\n",
    "\n",
    "\n",
    "layers = [16,32,16]\n",
    "activation = 'LeakyReLU'\n",
    "ode_method = 'rk4'\n",
    "n_aug=2\n",
    "#sde_scales=None\n",
    "sde_scales = len(groups)*[0.1] # if use dopri5 or any adaptative solver, one needs to increase the number of scales, e.g. (len(groups)+10)*[0.2]\n",
    "\n",
    "if use_geo:\n",
    "    geoemb = dae.encoder\n",
    "    if use_cuda:\n",
    "        geoemb = geoemb.cuda()\n",
    "else:\n",
    "    geoemb=None\n",
    "if use_dae:\n",
    "    autoencoder = dae\n",
    "    if use_cuda:\n",
    "        autoencoder = autoencoder.cuda()\n",
    "else:\n",
    "    autoencoder=None\n",
    "    \n",
    "if not small_model:\n",
    "    model = make_model(model_features, [32, 64, 128, 64, 32], activation=activation)\n",
    "else:\n",
    "    model = make_model(model_features, layers, activation=activation,method=ode_method, rtol=0.001,atol=0.001,scales=sde_scales, n_aug=n_aug)\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_with_replacement = False\n",
    "sample_size=(60, )\n",
    "n_samples=1\n",
    "\n",
    "n_local_epochs = 30\n",
    "n_epochs = 0\n",
    "n_post_local_epochs = 0\n",
    "\n",
    "n_batches = 20\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "criterion_name = 'ot'\n",
    "if criterion_name == 'mmd':\n",
    "    criterion = MMD_loss()\n",
    "else:\n",
    "    criterion = OT_loss()\n",
    "\n",
    "local_losses = {f'{t0}:{t1}':[] for (t0, t1) in steps}\n",
    "batch_losses = []\n",
    "globe_losses = []\n",
    "\n",
    "\n",
    "use_local_density = False\n",
    "\n",
    "\n",
    "n_points = 100\n",
    "n_trajectories = 100\n",
    "n_bins = 100\n",
    "\n",
    "add_noise = False\n",
    "noise_scale = 0.09\n",
    "use_gaussian = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'phate_dims': phate_dims,\n",
    "    'round_labels': round_labels,\n",
    "    'use_gaussian': use_gaussian,\n",
    "    'add_noise_directly': add_noise_directly,\n",
    "    'add_noise_after_phate': add_noise_after_phate,\n",
    "    'scale_factor': scale_factor,\n",
    "    'use_cuda': use_cuda,\n",
    "    'emb_features': emb_features,\n",
    "    'model_features': model_features,\n",
    "    'small_model': small_model,\n",
    "    'exp_name': exp_name,\n",
    "    'groups': groups,\n",
    "    'steps': steps,\n",
    "    'sample_with_replacement': sample_with_replacement,\n",
    "    'sample_size': sample_size,\n",
    "    'use_geo': use_geo,\n",
    "    'n_local_epochs': n_local_epochs,\n",
    "    'n_epochs': n_epochs,\n",
    "    'n_post_local_epochs': n_post_local_epochs,\n",
    "    'n_batches': n_batches,\n",
    "    'criterion_name': criterion_name,\n",
    "    'hold_one_out': hold_one_out,\n",
    "    'hinge_value': hinge_value,\n",
    "    'use_density_loss': use_density_loss,\n",
    "    'use_local_density': use_local_density,\n",
    "    'n_points': n_points,\n",
    "    'n_trajectories': n_trajectories,\n",
    "    'n_bins': n_bins,\n",
    "    'add_noise': add_noise,\n",
    "    'noise_scale': noise_scale,\n",
    "    'use_gaussian': use_gaussian,\n",
    "    'autoencoder': autoencoder,\n",
    "    'n_samples': n_samples,\n",
    "    'activation': activation,\n",
    "    'layer': layers,\n",
    "    'ode_solver': ode_method,\n",
    "    'lambda_density':lambda_density,\n",
    "    'top_k':top_k,\n",
    "    'use_dae': use_dae,\n",
    "    'sde_scales': sde_scales,\n",
    "    'n_augmented_ode': n_aug,\n",
    "    'hold_out':hold_out,\n",
    "    'encoder_layers': encoder_layers,\n",
    "    'n_epochs_emb': n_epochs_emb,\n",
    "    'samples_size_emb': samples_size_emb,\n",
    "    'recon_emb': recon_emb,\n",
    "    'dist': dist, \n",
    "    'use_penalty_energy':use_penalty_energy,\n",
    "    'lambda_energy':lambda_energy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir, logger = setup_exp(RES_DIR, opts, exp_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(n_local_epochs), desc='Pretraining Epoch'):\n",
    "    l_loss, b_loss, g_loss = train(\n",
    "        model, df, groups, optimizer, n_batches, \n",
    "        criterion = criterion, use_cuda = use_cuda,\n",
    "        local_loss=True, global_loss=False, apply_losses_in_time=True,\n",
    "        hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "        hinge_value=hinge_value,\n",
    "        use_density_loss = use_density_loss, use_local_density = use_local_density,       \n",
    "        top_k = top_k, lambda_density = lambda_density,  lambda_density_local = 1.0, \n",
    "        geo_emb = geoemb, use_emb = use_geo, sample_size=sample_size, \n",
    "        sample_with_replacement=sample_with_replacement, logger=logger, autoencoder=autoencoder, n_samples=n_samples,\n",
    "        add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian,use_penalty=use_penalty_energy,lambda_energy=lambda_energy\n",
    "    )\n",
    "    for k, v in l_loss.items():  \n",
    "        local_losses[k].extend(v)\n",
    "    batch_losses.extend(b_loss)\n",
    "    globe_losses.extend(g_loss)\n",
    "    \n",
    "for epoch in tqdm(range(n_epochs), desc='Epoch'):\n",
    "    l_loss, b_loss, g_loss = train(\n",
    "        model, df, groups, optimizer, n_batches, \n",
    "        criterion = criterion, use_cuda = use_cuda,\n",
    "        local_loss=False, global_loss=True, apply_losses_in_time=True,\n",
    "        hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "        hinge_value=hinge_value,\n",
    "        use_density_loss = use_density_loss, use_local_density = use_local_density,       \n",
    "        top_k = top_k, lambda_density = lambda_density, lambda_density_local = 1.0, \n",
    "        geo_emb =  geoemb, use_emb = use_geo, sample_size=sample_size, \n",
    "        sample_with_replacement=sample_with_replacement, logger=logger, autoencoder=autoencoder, n_samples=n_samples,\n",
    "        add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian,use_penalty=use_penalty_energy,lambda_energy=lambda_energy\n",
    "    )\n",
    "\n",
    "    for k, v in l_loss.items():  \n",
    "        local_losses[k].extend(v)\n",
    "    batch_losses.extend(b_loss)\n",
    "    globe_losses.extend(g_loss)\n",
    "    \n",
    "for epoch in tqdm(range(n_post_local_epochs), desc='Posttraining Epoch'):\n",
    "    l_loss, b_loss, g_loss = train(\n",
    "        model, df, groups, optimizer, n_batches, \n",
    "        criterion = criterion, use_cuda = use_cuda,\n",
    "        local_loss=True, global_loss=False, apply_losses_in_time=True,\n",
    "        hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "        hinge_value=hinge_value,\n",
    "        use_density_loss = use_density_loss, use_local_density = use_local_density,       \n",
    "        top_k = top_k, lambda_density = lambda_density,  lambda_density_local = 1.0, \n",
    "        geo_emb =  geoemb, use_emb = use_geo, sample_size=sample_size, \n",
    "        sample_with_replacement=sample_with_replacement, logger=logger, autoencoder=autoencoder, n_samples=n_samples,\n",
    "        add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian,use_penalty=use_penalty_energy,lambda_energy=lambda_energy\n",
    "    )\n",
    "    for k, v in l_loss.items():  \n",
    "        local_losses[k].extend(v)\n",
    "    batch_losses.extend(b_loss)\n",
    "    globe_losses.extend(g_loss)\n",
    "run_time = time.time() - start_time + run_time_geo if use_geo or use_dae else time.time() - start_time\n",
    "logger.info(f'Total run time: {np.round(run_time, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(\n",
    "    local_losses, batch_losses, globe_losses, \n",
    "    save=True, path=exp_dir, file='losses.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, trajectories = generate_plot_data(\n",
    "    model, df, n_points, n_trajectories, n_bins=100, \n",
    "    sample_with_replacement=sample_with_replacement, use_cuda=use_cuda, samples_key='samples', autoencoder=autoencoder\n",
    ")\n",
    "if autoencoder is not None:\n",
    "    if use_cuda:\n",
    "        generated, trajectories = torch.Tensor(generated).cuda(), torch.Tensor(trajectories).cuda()\n",
    "    else:\n",
    "        generated, trajectories = torch.Tensor(generated), torch.Tensor(trajectories)\n",
    "    generated, trajectories = autoencoder.decoder(generated).detach().cpu(), autoencoder.decoder(trajectories).detach().cpu() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparision(\n",
    "    df, generated, trajectories,\n",
    "    palette = 'viridis', df_time_key='samples',\n",
    "    save=True, path=exp_dir, file='2d_comparision.png',\n",
    "    x='d1', y='d2', z='d3', is_3d=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the trajectories and generated points\n",
    "np.save(os.path.join(exp_dir,'trajectories_leave{}.npy'.format(hold_out)),trajectories)\n",
    "np.save(os.path.join(exp_dir,'generated_leave{}.npy'.format(hold_out)),generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary fix for the logger\n",
    "import logging\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklab-toy-tjnet",
   "language": "python",
   "name": "sklab-toy-tjnet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
