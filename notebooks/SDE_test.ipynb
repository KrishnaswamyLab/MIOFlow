{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29493675",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a641d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MIOFlow.losses import MMD_loss, OT_loss, Density_loss, Local_density_loss\n",
    "from MIOFlow.utils import group_extract, sample, to_np, generate_steps\n",
    "from MIOFlow.models import ToyModel, make_model, Autoencoder, ToySDEModel\n",
    "from MIOFlow.plots import plot_comparision, plot_losses\n",
    "from MIOFlow.train import train, train_ae\n",
    "from MIOFlow.constants import ROOT_DIR, DATA_DIR, NTBK_DIR, IMGS_DIR, RES_DIR\n",
    "from MIOFlow.datasets import (\n",
    "    make_diamonds, make_swiss_roll, make_tree, make_eb_data, \n",
    "    make_dyngen_data, relabel_data\n",
    ")\n",
    "from MIOFlow.ode import NeuralODE, ODEF\n",
    "from MIOFlow.geo import DiffusionDistance, old_DiffusionDistance\n",
    "from MIOFlow.exp import setup_exp\n",
    "from MIOFlow.eval import generate_plot_data\n",
    "\n",
    "import os, pandas as pd, numpy as np, \\\n",
    "    seaborn as sns, matplotlib as mpl, matplotlib.pyplot as plt, \\\n",
    "    torch, torch.nn as nn\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from phate import PHATE\n",
    "\n",
    "# for geodesic learning\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f92d08",
   "metadata": {},
   "source": [
    "# Run Petals Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84506e2",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e7ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phate_dims = None\n",
    "round_labels=None\n",
    "use_gaussian=None\n",
    "add_noise_directly=None\n",
    "add_noise_after_phate=None\n",
    "scale_factor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9d35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_diamonds()\n",
    "sns.scatterplot(data=df, x='d1', y='d2', hue='samples', palette='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d87286",
   "metadata": {},
   "source": [
    "## Train autoencoder or the geodesic embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "groups = sorted(df.samples.unique())\n",
    "\n",
    "model_features = len(df.columns) - 1\n",
    "encoder_layers = [model_features,8,8]\n",
    "\n",
    "dae = Autoencoder(\n",
    "    encoder_layers = encoder_layers,\n",
    "    decoder_layers = [32,8,model_features],\n",
    "    activation='ReLU'\n",
    ")\n",
    "optimizer = torch.optim.AdamW(dae.parameters())\n",
    "dae.cuda() if use_cuda else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a357746",
   "metadata": {},
   "source": [
    "dist=None and recon=True for DAE or dist=DiffusionDistance(knn=40,t_max=3) recon=False for geo embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af045885",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_DiffusionDistance(RBF(5.0),t_max=3),DiffusionDistance(knn=40,t_max=3,symmetrize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90704e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time_geo = time.time()\n",
    "losses = train_ae(\n",
    "        dae, df, groups, optimizer, n_epochs=1000, sample_size=(30,),\n",
    "    noise_min_scale=0.001, noise_max_scale=0.02, dist=old_DiffusionDistance(RBF(0.5),t_max=5), recon=False\n",
    ")\n",
    "run_time_geo = time.time() - start_time_geo\n",
    "print(run_time_geo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72398857",
   "metadata": {},
   "source": [
    "## Or load a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e87df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "load = False\n",
    "if load:\n",
    "    with open(os.path.expanduser('~/Downloads/dyngen-df.pkl'), 'rb') as f:\n",
    "        df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df9ef7",
   "metadata": {},
   "source": [
    "## Specify parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c06172",
   "metadata": {},
   "source": [
    "Note: if we trained geo and reconstruction at the same time, then we use geo even if 'geo_emb=None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a667fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "exp_name = 'sde_petals'\n",
    "\n",
    "use_geo = False\n",
    "use_dae = False\n",
    "use_density_loss = False\n",
    "\n",
    "lambda_density = 20\n",
    "top_k=5\n",
    "hinge_value = 0.01\n",
    "\n",
    "small_model = True\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "emb_features = encoder_layers[-1] if use_dae else len(df.columns) - 1\n",
    "groups = sorted(df.samples.unique())\n",
    "\n",
    "layers_small = [16,32,16]\n",
    "activation = 'LeakyReLU'\n",
    "ode_method = 'euler'#'rk4'\n",
    "n_aug=2\n",
    "sde_scales=None\n",
    "#sde_scales = len(groups)*[0.2] # if use dopri5 or any adaptative solver, one needs to increase the number of scales, e.g. (len(groups)+10)*[0.2]\n",
    "\n",
    "if use_geo:\n",
    "    geoemb = dae.encoder\n",
    "    if use_cuda:\n",
    "        geoemb = geoemb.cuda()\n",
    "else:\n",
    "    geoemb=None\n",
    "if use_dae:\n",
    "    autoencoder = dae\n",
    "    if use_cuda:\n",
    "        autoencoder = autoencoder.cuda()\n",
    "else:\n",
    "    autoencoder=None\n",
    "    \n",
    "if not small_model:\n",
    "    model = make_model(emb_features, [32, 64, 128, 64, 32], activation=activation, which='sde')\n",
    "else:\n",
    "    model = make_model(\n",
    "        emb_features, layers_small, \n",
    "        which='sde',\n",
    "        activation=activation, method=ode_method, \n",
    "        rtol=0.001,atol=0.001, scales=sde_scales, n_aug=n_aug\n",
    "    )\n",
    "if use_cuda:\n",
    "    model.to(torch.device('cuda'))\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = sorted(df.samples.unique())\n",
    "steps = generate_steps(groups)\n",
    "\n",
    "sample_with_replacement = False\n",
    "sample_size=(60, )\n",
    "n_samples=1\n",
    "\n",
    "n_local_epochs = 20\n",
    "n_epochs = 0\n",
    "n_post_local_epochs = 0\n",
    "\n",
    "n_batches = 20\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "criterion_name = 'ot'\n",
    "if criterion_name == 'mmd':\n",
    "    criterion = MMD_loss()\n",
    "else:\n",
    "    criterion = OT_loss()\n",
    "\n",
    "hold_one_out = False\n",
    "hold_out = 'random' #'random' or time \n",
    "\n",
    "local_losses = {f'{t0}:{t1}':[] for (t0, t1) in steps}\n",
    "batch_losses = []\n",
    "globe_losses = []\n",
    "\n",
    "\n",
    "use_local_density = False\n",
    "\n",
    "\n",
    "n_points = 100\n",
    "n_trajectories = 100\n",
    "n_bins = 100\n",
    "\n",
    "add_noise = False\n",
    "noise_scale = 0.09\n",
    "use_gaussian = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    'phate_dims': phate_dims,\n",
    "    'round_labels': round_labels,\n",
    "    'use_gaussian': use_gaussian,\n",
    "    'add_noise_directly': add_noise_directly,\n",
    "    'add_noise_after_phate': add_noise_after_phate,\n",
    "    'scale_factor': scale_factor,\n",
    "    'use_cuda': use_cuda,\n",
    "    'emb_features': emb_features,\n",
    "    'model_features': model_features,\n",
    "    'small_model': small_model,\n",
    "    'exp_name': exp_name,\n",
    "    'groups': groups,\n",
    "    'steps': steps,\n",
    "    'sample_with_replacement': sample_with_replacement,\n",
    "    'sample_size': sample_size,\n",
    "    'use_geo': use_geo,\n",
    "    'n_local_epochs': n_local_epochs,\n",
    "    'n_epochs': n_epochs,\n",
    "    'n_post_local_epochs': n_post_local_epochs,\n",
    "    'n_batches': n_batches,\n",
    "    'criterion_name': criterion_name,\n",
    "    'hold_one_out': hold_one_out,\n",
    "    'hold_out': hold_out,\n",
    "    'hinge_value': hinge_value,\n",
    "    'use_density_loss': use_density_loss,\n",
    "    'use_local_density': use_local_density,\n",
    "    'n_points': n_points,\n",
    "    'n_trajectories': n_trajectories,\n",
    "    'n_bins': n_bins,\n",
    "    'add_noise': add_noise,\n",
    "    'noise_scale': noise_scale,\n",
    "    'use_gaussian': use_gaussian,\n",
    "    'autoencoder': autoencoder,\n",
    "    'n_samples': n_samples,\n",
    "    'activation': activation,\n",
    "    'layer': layers_small,\n",
    "    'ode_solver': ode_method,\n",
    "    'lambda_density':lambda_density,\n",
    "    'top_k':top_k,\n",
    "    'use_dae': use_dae,\n",
    "    'sde_scales': sde_scales,\n",
    "    'n_augmented_ode': n_aug\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a775b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir, logger = setup_exp(RES_DIR, opts, exp_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(n_local_epochs), desc='Pretraining Epoch'):\n",
    "    l_loss, b_loss, g_loss = train(\n",
    "        model, df, groups, optimizer, n_batches, \n",
    "        criterion = criterion, use_cuda = use_cuda,\n",
    "        local_loss=True, global_loss=False, apply_losses_in_time=True,\n",
    "        hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "        hinge_value=hinge_value,\n",
    "        use_density_loss = use_density_loss, use_local_density = use_local_density,       \n",
    "        top_k = top_k, lambda_density = lambda_density,  lambda_density_local = 1.0, \n",
    "        geo_emb = geoemb, use_emb = use_geo, sample_size=sample_size, \n",
    "        sample_with_replacement=sample_with_replacement, logger=logger, autoencoder=autoencoder, n_samples=n_samples,\n",
    "        add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian\n",
    "    )\n",
    "    for k, v in l_loss.items():  \n",
    "        local_losses[k].extend(v)\n",
    "    batch_losses.extend(b_loss)\n",
    "    globe_losses.extend(g_loss)\n",
    "    \n",
    "for epoch in tqdm(range(n_epochs), desc='Epoch'):\n",
    "    l_loss, b_loss, g_loss = train(\n",
    "        model, df, groups, optimizer, n_batches, \n",
    "        criterion = criterion, use_cuda = use_cuda,\n",
    "        local_loss=False, global_loss=True, apply_losses_in_time=True,\n",
    "        hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "        hinge_value=hinge_value,\n",
    "        use_density_loss = use_density_loss, use_local_density = use_local_density,       \n",
    "        top_k = top_k, lambda_density = lambda_density, lambda_density_local = 1.0, \n",
    "        geo_emb =  geoemb, use_emb = use_geo, sample_size=sample_size, \n",
    "        sample_with_replacement=sample_with_replacement, logger=logger, autoencoder=autoencoder, n_samples=n_samples,\n",
    "        add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian\n",
    "    )\n",
    "\n",
    "    for k, v in l_loss.items():  \n",
    "        local_losses[k].extend(v)\n",
    "    batch_losses.extend(b_loss)\n",
    "    globe_losses.extend(g_loss)\n",
    "    \n",
    "for epoch in tqdm(range(n_post_local_epochs), desc='Posttraining Epoch'):\n",
    "    l_loss, b_loss, g_loss = train(\n",
    "        model, df, groups, optimizer, n_batches, \n",
    "        criterion = criterion, use_cuda = use_cuda,\n",
    "        local_loss=True, global_loss=False, apply_losses_in_time=True,\n",
    "        hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "        hinge_value=hinge_value,\n",
    "        use_density_loss = use_density_loss, use_local_density = use_local_density,       \n",
    "        top_k = top_k, lambda_density = lambda_density,  lambda_density_local = 1.0, \n",
    "        geo_emb =  geoemb, use_emb = use_geo, sample_size=sample_size, \n",
    "        sample_with_replacement=sample_with_replacement, logger=logger, autoencoder=autoencoder, n_samples=n_samples,\n",
    "        add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian\n",
    "    )\n",
    "    for k, v in l_loss.items():  \n",
    "        local_losses[k].extend(v)\n",
    "    batch_losses.extend(b_loss)\n",
    "    globe_losses.extend(g_loss)\n",
    "run_time = time.time() - start_time + run_time_geo if use_geo else time.time() - start_time\n",
    "print(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 465.36s/it * 10 iterations ~ 70 minutes\n",
    "# try on dyngen once\n",
    "# nn for g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(\n",
    "    local_losses, batch_losses, globe_losses, \n",
    "    save=True, path=exp_dir, file='losses.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b345222",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated, trajectories = generate_plot_data(\n",
    "    model, df, n_points, n_trajectories, n_bins=100, \n",
    "    sample_with_replacement=sample_with_replacement, use_cuda=use_cuda, samples_key='samples', autoencoder=autoencoder\n",
    ")\n",
    "if autoencoder is not None:\n",
    "    if use_cuda:\n",
    "        generated, trajectories = torch.Tensor(generated).cuda(), torch.Tensor(trajectories).cuda()\n",
    "    else:\n",
    "        generated, trajectories = torch.Tensor(generated), torch.Tensor(trajectories)\n",
    "    generated, trajectories = autoencoder.decoder(generated).detach().cpu(), autoencoder.decoder(trajectories).detach().cpu() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparision(\n",
    "    df, generated, trajectories,\n",
    "    palette = 'viridis', df_time_key='samples',\n",
    "    save=True, path=exp_dir, file='2d_comparision.png',\n",
    "    x='d1', y='d2', z='d3', is_3d=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421385d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
