{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details\n",
    "output-file: eval.html\n",
    "title: Eval\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch, numpy as np\n",
    "from MIOFlow.utils import sample, to_np\n",
    "from MIOFlow.models import GrowthRateModel\n",
    "\n",
    "def generate_points(\n",
    "    model, df, n_points=100, \n",
    "    sample_with_replacement=False, use_cuda=False, \n",
    "    samples_key='samples', sample_time=None, autoencoder=None, recon=False\n",
    "):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.\n",
    "            See `MIOFlow.ode` for more.\n",
    "        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.\n",
    "        n_points (int): Number of points to generate.\n",
    "        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling\n",
    "            initial timepoint.\n",
    "        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.\n",
    "        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.\n",
    "        sample_time (list | None): Defaults to `None`. If `None` uses the group numbers in order as the \n",
    "            timepoints as specified in the column `df[samples_key]`.\n",
    "        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.\n",
    "        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.\n",
    "    Returns:\n",
    "    ----------\n",
    "        generated (float[float[]]): a list with shape `(len(sample_time), n_points, len(df.columns) - 1)`\n",
    "            of the generated points.\n",
    "    '''\n",
    "    to_torch = True #if use_cuda else False\n",
    "\n",
    "    groups = sorted(df[samples_key].unique())\n",
    "    if sample_time is None:\n",
    "        sample_time = groups\n",
    "    data_t0 = sample(\n",
    "        df, np.min(groups), size=(n_points, ), \n",
    "        replace=sample_with_replacement, to_torch=to_torch, use_cuda=use_cuda\n",
    "    )\n",
    "    if autoencoder is not None and recon:\n",
    "        data_t0 = torch.Tensor(data_t0).float()\n",
    "        data_t0 = autoencoder.encoder(data_t0)\n",
    "        \n",
    "    time =  torch.Tensor(sample_time).cuda() if use_cuda else torch.Tensor(sample_time)\n",
    "    if isinstance(model, GrowthRateModel):\n",
    "        generated, _ = model(data_t0, time, return_whole_sequence=True)\n",
    "    else:\n",
    "        generated = model(data_t0, time, return_whole_sequence=True)\n",
    "    if autoencoder is not None and recon:\n",
    "        generated = autoencoder.decoder(generated)\n",
    "    return to_np(generated)\n",
    "\n",
    "def generate_trajectories(\n",
    "    model, df, n_trajectories=30, n_bins=100, \n",
    "    sample_with_replacement=False, use_cuda=False, samples_key='samples',autoencoder=None, recon=False\n",
    "):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.\n",
    "            See `MIOFlow.ode` for more.\n",
    "        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.\n",
    "        n_trajectories (int): Number of trajectories to generate.\n",
    "        n_bins (int): Number of bins to use for the trajectories. More makes it smoother. Defaults to `100`.\n",
    "        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling\n",
    "            initial timepoint.\n",
    "        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.\n",
    "        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.\n",
    "        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.\n",
    "        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.\n",
    "    Returns:\n",
    "    ----------\n",
    "        trajectories (float[float[]]): a list with shape `(n_bins, n_points, len(df.columns) - 1)`\n",
    "            of the generated trajectories.\n",
    "    '''\n",
    "    groups = sorted(df[samples_key].unique())\n",
    "    sample_time = np.linspace(np.min(groups), np.max(groups), n_bins)\n",
    "    trajectories = generate_points(model, df, n_trajectories, sample_with_replacement, use_cuda, samples_key, sample_time,autoencoder=autoencoder, recon=recon)\n",
    "    return trajectories\n",
    "    \n",
    "def generate_plot_data(\n",
    "    model, df, n_points, n_trajectories, n_bins, \n",
    "    sample_with_replacement=False, use_cuda=False, samples_key='samples',\n",
    "    logger=None, autoencoder=None, recon=False\n",
    "):\n",
    "    '''\n",
    "    Arguments:\n",
    "    ----------\n",
    "        model (torch.nn.Module): Trained network with the property `ode` corresponding to a `NeuralODE(ODEF())`.\n",
    "            See `MIOFlow.ode` for more.\n",
    "        df (pd.DataFrame): DataFrame containing a column for the timepoint samples and the rest of the data.\n",
    "        n_points (int): Number of points to generate.\n",
    "        n_trajectories (int): Number of trajectories to generate.\n",
    "        n_bins (int): Number of bins to use for the trajectories. More makes it smoother. Defaults to `100`.\n",
    "        sample_with_replacement (bool): Defaults to `False`. Whether or not to use replacement when sampling\n",
    "            initial timepoint.\n",
    "        use_cuda (bool): Defaults to `False`. Whether or not to use cuda.\n",
    "        samples_key (str): Defaults to `'samples'`. The column in the `df` which has the timepoint groups.\n",
    "        autoencoder (nn.Module|NoneType): Default to None, the trained autoencoder.\n",
    "        recon (bool): Default to 'False', whether to use the autoencoder for reconstruction.\n",
    "    Returns:\n",
    "    ----------\n",
    "        points (float[float[]]): a list with shape `(len(df[sample_key].unique()), n_points, len(df.columns) - 1)`\n",
    "            of the generated points.\n",
    "        trajectories (float[float[]]): a list with shape `(n_bins, n_points, len(df.columns) - 1)`\n",
    "            of the generated trajectories.\n",
    "    '''\n",
    "    if logger: logger.info(f'Generating points')\n",
    "    points = generate_points(model, df, n_points, sample_with_replacement, use_cuda, samples_key, None, autoencoder=autoencoder, recon=recon)\n",
    "    if logger: logger.info(f'Generating trajectories')\n",
    "    trajectories = generate_trajectories(model, df, n_trajectories, n_bins, sample_with_replacement, use_cuda, samples_key, autoencoder=autoencoder, recon=recon)\n",
    "    return points, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, logging, sklearn, pandas as pd, numpy as np\n",
    "from typing import Union\n",
    "try:\n",
    "    from typing import Literal\n",
    "except ImportError:\n",
    "    from typing_extensions import Literal\n",
    "from MIOFlow.utils import generate_steps\n",
    "\n",
    "def get_points_from_trajectories(\n",
    "    n_groups:int, \n",
    "    trajectories:Union[np.ndarray, list], \n",
    "    how:Union[Literal['start'], Literal['middle'], Literal['end']]='start', \n",
    "    logger:logging.Logger=None\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Arguments:\n",
    "        n_groups (int): how many time points there are total. \n",
    "        trajectories (np.ndarray | list): A list with three dimensions that correspond to:\n",
    "            `(n_bins, n_points, n_dims)`, where `n_points` are the number of points / trajectories there are,\n",
    "            `n_bins` correponds to how the trajectories were smoothed via binning (e.g. if there were 5 total\n",
    "            time points one might have 100 bins to draw a smoother line when plotting), and `n_dims` are the\n",
    "            number of dimensions the points are in (e.g. 2).\n",
    "        how (str): Defaults to `'start'`. How to extract the point for the binned trajectories.\n",
    "            If `'start'` takes the first point in the time window. If `'middle'` takes the floored averaged.\n",
    "            If `'end'` takes the the last point in the time window.\n",
    "        logger (logging.Logger): Defaults to `None`.\n",
    "    Returns:\n",
    "        points (np.ndarray): Points at the corresponding indicices. If `trajectories` has shape:\n",
    "            `(n_bins, n_points, n_dims)`, this will have shape `(n_groups, n_points, n_dims)`.\n",
    "    '''\n",
    "    # Value handling\n",
    "    _valid_how = 'start middle end'.split()\n",
    "    if how not in _valid_how:\n",
    "        raise ValueError(f'Unknown option specified for `how`. Must be in {_valid_how}')    \n",
    "        \n",
    "    if not isinstance(trajectories, np.ndarray):\n",
    "        trajectories = np.array(trajectories)\n",
    "        \n",
    "    trajectories = np.transpose(trajectories, axes=(1, 0, 2))\n",
    "    (n_points, n_bins, n_dims) = trajectories.shape\n",
    "    if logger: \n",
    "        logger.info(\n",
    "            f'Given trajectories object with {n_points} points of {n_bins} '\n",
    "            f'bins in {n_dims} dimensions.'\n",
    "        )\n",
    "    \n",
    "    parts = np.linspace(0, n_bins, n_groups + 1).astype(int).tolist()\n",
    "    steps = generate_steps(parts)\n",
    "    results = []\n",
    "    for step in steps:\n",
    "        time_window = trajectories[:, slice(*step)] \n",
    "        if how == 'start':\n",
    "            results.append(time_window[:, 0, :])\n",
    "        elif how == 'middle':\n",
    "            idx = int(np.floor(n_bins / n_groups / 2))\n",
    "            results.append(time_window[:, idx, :])\n",
    "        elif how == 'end':\n",
    "            results.append(time_window[:, -1, :])\n",
    "        else:\n",
    "            raise NotImplementedError(f'how={how} not implemented')        \n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def calculate_nn(\n",
    "    df:pd.DataFrame,\n",
    "    generated:Union[np.ndarray, list]=None,\n",
    "    trajectories:Union[np.ndarray, list]=None,\n",
    "    compare_to:Union[Literal['time'], Literal['any']]='time',\n",
    "    how:Union[Literal['start'], Literal['middle'], Literal['end']]='start',     \n",
    "    k:int=1,\n",
    "    groups:Union[np.ndarray, list]=None,\n",
    "    sample_key:str='samples',\n",
    "    method:str='mean',\n",
    "    logger:logging.Logger=None\n",
    ") -> float:\n",
    "    '''\n",
    "    Arguments:\n",
    "        df (pd.DataFrame): DataFrame containing all points and time sample specified by `sample_key`\n",
    "        generated (np.ndarray | list): A list of the generate points with shape \n",
    "            `(n_groups, n_points, n_dims)`, where `n_groups` is the total number of time indicies\n",
    "            as specified in `groups`.\n",
    "        trajectories (np.ndarray | list): A list with three dimensions that correspond to:\n",
    "            `(n_bins, n_points, n_dims)`, where `n_points` are the number of points / trajectories there are,\n",
    "            `n_bins` correponds to how the trajectories were smoothed via binning (e.g. if there were 5 total\n",
    "            time points one might have 100 bins to draw a smoother line when plotting), and `n_dims` are the\n",
    "            number of dimensions the points are in (e.g. 2).\n",
    "        compare_to (str): Defaults to `'time'`. Determines points to use for KNN. If `'time'` will only \n",
    "            consider points at the same time index. If `'any'` will search for nearest points regardless\n",
    "            of time.\n",
    "        how (str): Defaults to `'start'`. How to extract the point for the binned trajectories.\n",
    "            If `'start'` takes the first point in the time window. If `'middle'` takes the floored averaged.\n",
    "            If `'end'` takes the the last point in the time window.\n",
    "        k (int): Defaults to `1`. Number of points to compare predicted points in `generated` \n",
    "            or `trajectories` to.\n",
    "        groups (np.ndarray | list): Defaults to `None` and will be extracted from `df` is not provided. \n",
    "            The sorted unique values of time samples from `df` as specified by `sample_key`.\n",
    "        sample_key (str): Defaults to `'samples'`. The column in `df` which corresponds to the time index.\n",
    "        method (str): Defaults to `'mean'`. If `'mean'` returns the mean of the knn distances. If `'quartile'`\n",
    "            returns the mean of the worst (highest distances) quartile.\n",
    "        logger (logging.Logger): Defaults to `None`.\n",
    "    Returns:\n",
    "        mean_dist (float): mean distance of predicted points to the `n` nearest-neighbor points.\n",
    "    '''\n",
    "    _valid_compare_to = 'time any'.split()\n",
    "    if compare_to not in _valid_compare_to:\n",
    "        raise ValueError(f'Unknown option specified for `compare_to`. Must be in {_valid_compare_to}') \n",
    "        \n",
    "    _valid_how = 'start middle end'.split()\n",
    "    if how not in _valid_how:\n",
    "        raise ValueError(f'Unknown option specified for `how`. Must be in {_valid_how}')\n",
    "\n",
    "    _valid_method = 'mean quartile'.split()\n",
    "    if method not in _valid_method:\n",
    "        raise ValueError(f'Unknown option specified for `method`. Must be in {_valid_method}')\n",
    "        \n",
    "    if trajectories is None and generated is None:\n",
    "        raise ValueError(f'Either generated or trajectories must not be None!')\n",
    "        \n",
    "    if groups is None:\n",
    "        groups = sorted(df[sample_key].unique())\n",
    "    \n",
    "    if generated is None:\n",
    "        generated = get_points_from_trajectories(len(groups), trajectories, how, logger)\n",
    "    \n",
    "    distances = []    \n",
    "    for idx, time_sample in enumerate(sorted(groups)):            \n",
    "        pred_points = generated[idx]\n",
    "        \n",
    "        # NOTE: compare to points only at same time index\n",
    "        if compare_to == 'time':\n",
    "            true_points = df.groupby(sample_key).get_group(time_sample).drop(columns=sample_key).values\n",
    "        # NOTE: compare to any point\n",
    "        elif compare_to == 'any':\n",
    "            true_points = df.drop(columns=sample_key).values\n",
    "        else:            \n",
    "            raise NotImplementedError(f'compare_to={compare_to} not implemented')\n",
    "        true_points = true_points[:, :pred_points.shape[1]]\n",
    "        neigh = sklearn.neighbors.NearestNeighbors(n_neighbors=k)\n",
    "        neigh.fit(true_points)\n",
    "        dists, indicies = neigh.kneighbors(pred_points, return_distance=True)\n",
    "        distances.extend(dists.flatten().tolist())\n",
    "    \n",
    "    distances = np.array(distances)\n",
    "    if method == 'mean':      \n",
    "        return distances.mean()\n",
    "    elif method == 'quartile':\n",
    "        q1 = np.quantile(distances, 0.25)\n",
    "        q2 = np.quantile(distances, 0.50)\n",
    "        q3 = np.quantile(distances, 0.75)\n",
    "        \n",
    "        b1 = distances[np.where(distances < q1)]\n",
    "        b2 = distances[np.where((distances < q2) & (distances >= q1))]\n",
    "        b3 = distances[np.where((distances < q3) & (distances >= q2))]\n",
    "        b4 = distances[np.where(distances >= q3)]\n",
    "\n",
    "        return np.max([np.mean(b) for b in [b1, b2, b3, b4]])\n",
    "    else:\n",
    "        raise NotImplementedError(f'method={method} not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from MIOFlow.models import GrowthRateModel\n",
    "from MIOFlow.utils import (\n",
    "    to_np, get_groups_from_df, get_cell_types_from_df, \n",
    "    get_sample_n_from_df, get_times_from_groups\n",
    ")\n",
    "import seaborn as sns\n",
    "def generate_tjnet_trajectories(\n",
    "    model, df, n_bins=10, use_cuda=False, samples_key='samples', \n",
    "    autoencoder=None, recon=False, where='end', start=0\n",
    "):\n",
    "    '''\n",
    "    Arguments:\n",
    "    -----------\n",
    "        model (nn.Module): Trained MIOFlow model.\n",
    "\n",
    "        df (pd.DataFrame): DataFrame of shape (n_cells, dimensions + 1), where the extra column\n",
    "            stems from a samples column (column indicating the timepoint of the cell). \n",
    "            By default the samples column is assumed to be `\"samples\"`.\n",
    "        \n",
    "        n_bins (int): For each time point split it into `n_bins` for smoother trajectories.\n",
    "            If there are `t` time points then there will be `t * n_bins` total points.\n",
    "        \n",
    "        use_cuda (bool): Whether or not to use cuda for the model and autoencoder.\n",
    "        \n",
    "        samples_key (str): The name of the column in the `df` that corresponds to the time\n",
    "            samples. Defaults to `\"samples\"`. \n",
    "        \n",
    "        autoencoder (nn.Module) Trained Geodesic Autoencoder.\n",
    "        \n",
    "        recon (bool): Whether or not to use the `autoencoder` to reconstruct the output\n",
    "            space from the `model`.\n",
    "        \n",
    "        where (str): Choices are `\"start\"`, and `\"end\"`. Defaults to `\"end\"`. Whether or not\n",
    "            to start the trajectories at `t_0` (`\"start\"`) or `t_n` (`\"end\"`). \n",
    "    \n",
    "        start (int): Defaults to `0`. Where in `generate_tjnet_trajectories` the trajectories started.\n",
    "            This is used if attempting to generate outside of `t0`. Note this works relative to `where`.\n",
    "            E.g. if `where=\"end\"` and `start=0` then this is the same as `groups[-1]`.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "        trajectories (np.ndarray): Trajectories with shape (time, cells, dimensions)\n",
    "    '''\n",
    "    \n",
    "    _valid_where = 'start end'.split()\n",
    "    if where not in _valid_where:\n",
    "        raise ValueError(f'{where} not known. Should be one of {_valid_where}')\n",
    "    \n",
    "    groups = sorted(df[samples_key].unique())\n",
    "    \n",
    "    # times = groups\n",
    "    # if where == 'end':\n",
    "    #     times = times[::-1]\n",
    "    # times = times[start:]\n",
    "    \n",
    "    times = get_times_from_groups(groups, where, start)\n",
    "\n",
    "    # a, b = (np.max(groups), np.min(groups)) if where == 'end' else (np.min(groups), np.max(groups))    \n",
    "    a, b = (np.max(times), np.min(times)) if where == 'end' else (np.min(times), np.max(times))    \n",
    "    # n = -1 if where == 'end' else 0\n",
    "    n = 0 # because reversed if needed and prunned\n",
    "    \n",
    "    # sample_time = np.linspace(a, b, len(groups) * n_bins)\n",
    "    sample_time = np.linspace(a, b, len(times) * n_bins)\n",
    "    sample_time = torch.Tensor(sample_time)\n",
    "    \n",
    "    # data_tn = df[df.samples == groups[n]].drop(columns=samples_key).values    \n",
    "    data_tn = df[df.samples == times[n]].drop(columns=samples_key).values    \n",
    "    data_tn = torch.Tensor(data_tn).float()\n",
    "    \n",
    "    if use_cuda:\n",
    "        data_tn = data_tn.cuda()\n",
    "        sample_time = sample_time.cuda()\n",
    "            \n",
    "    if autoencoder is not None and recon:\n",
    "        data_tn = autoencoder.encoder(data_tn)\n",
    "    if isinstance(model, GrowthRateModel):\n",
    "        generated, _ = model(data_tn, sample_time, return_whole_sequence=True)\n",
    "    else:\n",
    "        generated = model(data_tn, sample_time, return_whole_sequence=True)\n",
    "    if autoencoder is not None and recon:\n",
    "        generated = autoencoder.decoder(generated)\n",
    "    generated = to_np(generated)\n",
    "    return generated\n",
    "\n",
    "\n",
    "def get_cell_indexes(\n",
    "    df, genes, trajectories, principal_components,\n",
    "    top_n=10, where='end', start=0, palette = 'viridis', \n",
    "    samples_key='samples',  samples=None,\n",
    "    cell_type_key=None, cell_types=None, use_cell_types=True\n",
    "):\n",
    "    '''\n",
    "    Notes:\n",
    "    -----------\n",
    "        - `samples` refers to the timepoint sample e.g. `samples == 1` should be Boolean array\n",
    "            corresponding to which rows in `df` that are of `t_1`.\n",
    "            \n",
    "        - `use_cell_types` determines the output shape of `top_idxs`. \n",
    "        \n",
    "            + `use_cell_types=True`: `top_idxs` is a nested dictionary with structure\n",
    "                ```\n",
    "                    {\n",
    "                        cell_type_0: {\n",
    "                            gene_0: [id_0, id_1, ... id_n]\n",
    "                        },\n",
    "                        ...\n",
    "                        cell_type_m: {...},\n",
    "                    }\n",
    "                ```\n",
    "                Where each id is a cell of the outer cell type with the highest expression of\n",
    "                the specified gene either at `t_0` (`where=\"start\"`) or `t_n` (`where=\"end\"`)\n",
    "                e.g. cell_type_0[gene_0][0] is the id of the top cell of cell type `cell_type_0`\n",
    "                expressing gene `gene_0`.\n",
    "                \n",
    "            + `use_cell_types=False`: `top_idxs` is a dictionary with structure\n",
    "                ```\n",
    "                    {                       \n",
    "                        gene_0: [id_0, id_1, ... id_n],\n",
    "                        gene_1: [id_0, id_1, ... id_n],\n",
    "                        ...\n",
    "                        gene_m: [id_0, id_1, ... id_n],                        \n",
    "                    }\n",
    "                ```\n",
    "                Where each id is a cell (of any cell type) that has the highest expression of the\n",
    "                specified gene either at `t_0` (`where=\"start\"`) or `t_n` (`where=\"end\"`).\n",
    "    \n",
    "    Arguments:\n",
    "    -----------\n",
    "        model (nn.Module): Trained MIOFlow model.\n",
    "\n",
    "        df (pd.DataFrame): DataFrame of shape (n_cells, n_genes), where the ordering of \n",
    "            the columns `n_genes` corresponds to the columns of `principle_components`.\n",
    "            It is assumed that the index of `df` are the cell types (but this need not be the case. \n",
    "            See `cell_types`). If there are additional columns (e.g. `samples_key`, `cell_type_key`)\n",
    "            should be after the gene columns.\n",
    "            \n",
    "        genes (np.ndarray | list): Genes of interest to determine which cell indexes to find.\n",
    "        \n",
    "        trajectories (np.ndarray): Trajectories with shape (time, cells, dimensions)\n",
    "        \n",
    "        principal_components (np.ndarray): The principle components with shape (dimensions, n_genes).\n",
    "            If used phate, can be obtained from `phate_operator.graph.data_pca.components_`\n",
    "        \n",
    "        top_n (int): Defaults to `10`. The number of cells to use per condition. If \n",
    "            `use_cell_types = False` this (conditions) will be the number of genes (`len(genes)`)\n",
    "            otherwise it will be the number of cell types.\n",
    "        \n",
    "        where (str): Choices are `\"start\"`, and `\"end\"`. Defaults to `\"end\"`. Whether or not\n",
    "            the trajectories start at `t_0` (`\"start\"`) or `t_n` (`\"end\"`). \n",
    "\n",
    "        start (int): Defaults to `0`. Where in `generate_tjnet_trajectories` the trajectories started.\n",
    "            This is used if attempting to generate outside of `t0`. Note this works relative to `where`.\n",
    "            E.g. if `where=\"end\"` and `start=0` then this is the same as `groups[-1]`.\n",
    "        \n",
    "        palette (str): A Matplotlib colormap. Defaults to `\"viridis\"`. \n",
    "        \n",
    "        samples_key (str): The name of the column in the `df` that corresponds to the time\n",
    "            samples. Defaults to `\"samples\"`. If `df[samples_key]` throws a `KeyError` \n",
    "            either because the `df` doesnt have this column in it or typo, will resort to\n",
    "            `samples` to determine this.\n",
    "                        \n",
    "        samples (np.ndarray | list): List of timepoints where each value corresponds to the \n",
    "            timepoint of the same row in `df`. Defaults to `None`.\n",
    "        \n",
    "        cell_type_key (str): The column name in the provided DataFrame `df` the corresponds to the \n",
    "            cell's cell types. Defaults to `None` which assumes the cell type is the index of the \n",
    "            `df i.e. `df.index`\n",
    "        \n",
    "        cell_types (np.ndarray | list): List of cell types to use from the provided DataFrame `df`.\n",
    "            Defaults to `None`. If `use_cell_types = True` will attempt to figure this out from\n",
    "            `cell_type_key`.\n",
    "        \n",
    "        use_cell_types (bool): Whether or not to use cell types.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "        genes (np.ndarray): List of genes similar to those the user passed into this function except\n",
    "            in order of the columns of the provided `df`. Any genes not found in the `df` put passed in\n",
    "            by the user will be removed.\n",
    "            \n",
    "        top_idxs (dict | dict[dict]): See notes. Dictionary or nested dictionary where leaf values are\n",
    "            indicies of cells corresponding to those expressing the highest amount of specified genes.\n",
    "            \n",
    "        inverse (np.nddary): Reconstructed gene space from `trajectories` and `principal_components`.\n",
    "            It has the shape (n_time * n_bins, n_cells, n_genes). See `generate_tjnet_trajectories`.\n",
    "        \n",
    "        colors (dict): Dictionary of either `{gene: color}` or `{cell_type: color}` depending on `use_cell_types`.\n",
    "    '''\n",
    "    # Test for valid start location\n",
    "    _valid_where = 'start end'.split()\n",
    "    if where not in _valid_where:\n",
    "        raise ValueError(f'{where} not known. Should be one of {_valid_where}')\n",
    "        \n",
    "    groups = get_groups_from_df(df, samples_key, samples)\n",
    "\n",
    "    # times = groups\n",
    "    # if where == 'end':\n",
    "    #     times = times[::-1]\n",
    "    # times = times[start:]\n",
    "\n",
    "    times = get_times_from_groups(groups, where, start)\n",
    "    \n",
    "    # Extract all of the cells at the specified either the start / end\n",
    "    n = -1 if where == 'end' else 0\n",
    "    # counts_n = get_sample_n_from_df(df, n, samples_key, samples, groups, drop_index=False)\n",
    "    counts_n = get_sample_n_from_df(df, 0, samples_key, samples, groups=times, drop_index=False)\n",
    "              \n",
    "    # Filter for only known genes\n",
    "    genes_mask = df.columns.isin(genes)\n",
    "    # Get genes in order\n",
    "    genes = df.columns[genes_mask]\n",
    "        \n",
    "    # Reconstruct full gene space (of just the genes we care about) \n",
    "    # from trajectories and principal components\n",
    "    inverse =  np.dot(trajectories, principal_components[:, genes_mask])\n",
    "                        \n",
    "    if use_cell_types:\n",
    "        # Try to correct for missing cell types if they are required\n",
    "        cell_types = get_cell_types_from_df(df, cell_type_key, cell_types)\n",
    "                \n",
    "        # Get name of cell_type_key column              \n",
    "        index = counts_n.columns[0] if cell_type_key is None else cell_type_key\n",
    "            \n",
    "        # Create colors for each cell type\n",
    "        cmap = sns.color_palette(palette, n_colors=len(cell_types))\n",
    "        colors = dict(zip(*[\n",
    "            cell_types,\n",
    "            [cmap[i] for i in range(len(cell_types))]\n",
    "        ]))\n",
    "        \n",
    "        # For each cell type and each gene get top_n cells of that cell type expressing that gene\n",
    "        top_idxs = {}\n",
    "        for cell_type in cell_types:\n",
    "            cells = counts_n[counts_n[index] == cell_type] \n",
    "            top_idxs[cell_type] = {}\n",
    "            for gene in genes:\n",
    "                top_idx = cells[gene].values.flatten().argsort()[-(top_n):]\n",
    "                top_idxs[cell_type][gene] = top_idx\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # Create colors for each gene\n",
    "        cmap = sns.color_palette(palette, n_colors=len(genes))\n",
    "        colors = dict(zip(*[\n",
    "            genes,\n",
    "            [cmap[i] for i in range(len(genes))]\n",
    "        ]))    \n",
    "            \n",
    "        # For each gene, get top_n cells expressing that gene    \n",
    "        top_idxs = {}\n",
    "        for gene in genes:\n",
    "            top_idx = counts_n[gene].values.flatten().argsort()[-(top_n):]\n",
    "            top_idxs[gene] = top_idx\n",
    "            \n",
    "        \n",
    "    return genes, top_idxs, inverse, colors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
