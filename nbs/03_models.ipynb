{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: models.html\n",
    "title: Models\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "from torch.nn  import functional as F \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class ToyODE(nn.Module):\n",
    "    \"\"\" \n",
    "    ODE derivative network\n",
    "    \n",
    "    feature_dims (int) default '5': dimension of the inputs, either in ambient space or embedded space.\n",
    "    layer (list of int) defaulf ''[64]'': the hidden layers of the network.\n",
    "    activation (torch.nn) default '\"ReLU\"': activation function applied in between layers.\n",
    "    scales (NoneType|list of float) default 'None': the initial scale for the noise in the trajectories. One scale per bin, add more if using an adaptative ODE solver.\n",
    "    n_aug (int) default '1': number of added dimensions to the input of the network. Total dimensions are features_dim + 1 (time) + n_aug. \n",
    "    \n",
    "    Method\n",
    "    forward (Callable)\n",
    "        forward pass of the ODE derivative network.\n",
    "        Parameters:\n",
    "        t (torch.tensor): time of the evaluation.\n",
    "        x (torch.tensor): position of the evalutation.\n",
    "        Return:\n",
    "        derivative at time t and position x.   \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_dims=5,\n",
    "        layers=[64],\n",
    "        activation='ReLU',\n",
    "        scales=None,\n",
    "        n_aug=2\n",
    "    ):\n",
    "        super(ToyODE, self).__init__()\n",
    "        steps = [feature_dims+1+n_aug, *layers, feature_dims]\n",
    "        pairs = zip(steps, steps[1:])\n",
    "\n",
    "        chain = list(itertools.chain(*list(zip(\n",
    "            map(lambda e: nn.Linear(*e), pairs), \n",
    "            itertools.repeat(getattr(nn, activation)())\n",
    "        ))))[:-1]\n",
    "\n",
    "        self.chain = chain\n",
    "        self.seq = (nn.Sequential(*chain))\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.tensor(scales, requires_grad=True).float()) if scales is not None else None\n",
    "        self.n_aug = n_aug        \n",
    "        \n",
    "    def forward(self, t, x): #NOTE the forward pass when we use torchdiffeq must be forward(self,t,x)\n",
    "        zero = torch.tensor([0]).cuda() if x.is_cuda else torch.tensor([0])\n",
    "        zeros = zero.repeat(x.size()[0],self.n_aug)\n",
    "        time = t.repeat(x.size()[0],1)\n",
    "        aug = torch.cat((x,time,zeros),dim=1)\n",
    "        x = self.seq(aug)\n",
    "        if self.alpha is not None:\n",
    "            z = torch.randn(x.size(),requires_grad=False).cuda() if x.is_cuda else torch.randn(x.size(),requires_grad=False)\n",
    "        dxdt = x + z*self.alpha[int(t-1)] if self.alpha is not None else x\n",
    "        return dxdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_model(\n",
    "    feature_dims=5,\n",
    "    layers=[64],\n",
    "    output_dims=5,\n",
    "    activation='ReLU',\n",
    "    which='ode',\n",
    "    method='rk4',\n",
    "    rtol=None,\n",
    "    atol=None,\n",
    "    scales=None,\n",
    "    n_aug=2,\n",
    "    noise_type='diagonal', sde_type='ito',\n",
    "    use_norm=False,\n",
    "    use_cuda=False,\n",
    "    in_features=2, out_features=2, gunc=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates the 'ode' model or 'sde' model or the Geodesic Autoencoder. \n",
    "    See the parameters of the respective classes.\n",
    "    \"\"\"\n",
    "    if which == 'ode':\n",
    "        ode = ToyODE(feature_dims, layers, activation,scales,n_aug)\n",
    "        model = ToyModel(ode,method,rtol, atol, use_norm=use_norm)\n",
    "    elif which == 'sde':\n",
    "        ode = ToyODE(feature_dims, layers, activation,scales,n_aug)\n",
    "        model = ToySDEModel(\n",
    "            ode, method, noise_type, sde_type,\n",
    "            in_features=in_features, out_features=out_features, gunc=gunc\n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        model = ToyGeo(feature_dims, layers, output_dims, activation)\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "from torch.nn  import functional as F \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Geodesic Autoencoder\n",
    "    \n",
    "    encoder_layers (list of int) default '[100, 100, 20]': encoder_layers[0] is the feature dimension, and encoder_layers[-1] the embedded dimension.\n",
    "    decoder_layers (list of int) defaulf '[20, 100, 100]': decoder_layers[0] is the embbeded dim and decoder_layers[-1] the feature dim.\n",
    "    activation (torch.nn) default '\"Tanh\"': activation function applied in between layers.\n",
    "    use_cuda (bool) default to False: Whether to use GPU or CPU.\n",
    "    \n",
    "    Method\n",
    "    encode\n",
    "        forward pass of the encoder\n",
    "        x (torch.tensor): observations\n",
    "        Return:\n",
    "        the encoded observations\n",
    "    decode\n",
    "        forward pass of the decoder\n",
    "        z (torch.tensor): embedded observations\n",
    "        Return:\n",
    "        the decoded observations\n",
    "    forward (Callable):\n",
    "        full forward pass, encoder and decoder\n",
    "        x (torch.tensor): observations\n",
    "        Return:\n",
    "        denoised observations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layers = [100, 100, 20],\n",
    "        decoder_layers = [20, 100, 100],\n",
    "        activation = 'Tanh',\n",
    "        use_cuda = False\n",
    "    ):        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        if decoder_layers is None:\n",
    "            decoder_layers = [*encoder_layers[::-1]]\n",
    "        device = 'cuda' if use_cuda else 'cpu'\n",
    "        \n",
    "        encoder_shapes = list(zip(encoder_layers, encoder_layers[1:]))\n",
    "        decoder_shapes = list(zip(decoder_layers, decoder_layers[1:]))\n",
    "        \n",
    "        encoder_linear = list(map(lambda a: nn.Linear(*a), encoder_shapes))\n",
    "        decoder_linear = list(map(lambda a: nn.Linear(*a), decoder_shapes))\n",
    "        \n",
    "        encoder_riffle = list(itertools.chain(*zip(encoder_linear, itertools.repeat(getattr(nn, activation)()))))[:-1]\n",
    "        encoder = nn.Sequential(*encoder_riffle).to(device)\n",
    "        \n",
    "        decoder_riffle = list(itertools.chain(*zip(decoder_linear, itertools.repeat(getattr(nn, activation)()))))[:-1]\n",
    "\n",
    "        decoder = nn.Sequential(*decoder_riffle).to(device)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        \n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural ODE\n",
    "        func (nn.Module): The network modeling the derivative.\n",
    "        method (str) defaulf '\"rk4\"': any methods from torchdiffeq.\n",
    "        rtol (NoneType | float): the relative tolerance of the ODE solver.\n",
    "        atol (NoneType | float): the absolute tolerance. of the ODE solver.\n",
    "        use_norm (bool): if True keeps the norm of func.\n",
    "        norm (list of torch.tensor): the norm of the derivative.\n",
    "        \n",
    "        Method\n",
    "        forward (Callable)\n",
    "            x (torch.tensor): the initial sample\n",
    "            t (torch.tensor) time points where we suppose x is from t[0]\n",
    "            return the last sample or the whole seq.      \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, method='rk4', rtol=None, atol=None, use_norm=False):\n",
    "        super(ToyModel, self).__init__()        \n",
    "        self.func = func\n",
    "        self.method = method\n",
    "        self.rtol=rtol\n",
    "        self.atol=atol\n",
    "        self.use_norm = use_norm\n",
    "        self.norm=[]\n",
    "\n",
    "    def forward(self, x, t, return_whole_sequence=False):\n",
    "\n",
    "        if self.use_norm:\n",
    "            for time in t: \n",
    "                self.norm.append(torch.linalg.norm(self.func(time,x)).pow(2))\n",
    "        if self.atol is None and self.rtol is None:\n",
    "            x = odeint(self.func,x ,t, method=self.method)\n",
    "        elif self.atol is not None and self.rtol is None:\n",
    "            x = odeint(self.func,x ,t, method=self.method, atol=self.atol)\n",
    "        elif self.atol is None and self.rtol is not None:\n",
    "            x = odeint(self.func,x ,t, method=self.method, rtol=self.rtol)\n",
    "        else: \n",
    "            x = odeint(self.func,x ,t, method=self.method, atol=self.atol, rtol=self.rtol)          \n",
    "       \n",
    "        x = x[-1] if not return_whole_sequence else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsde\n",
    "\n",
    "class ToySDEModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural SDE model\n",
    "        func (nn.Module): drift term.\n",
    "        genc (nn.Module): diffusion term.\n",
    "        method (str): method of the SDE solver.\n",
    "        \n",
    "        Method\n",
    "        forward (Callable)\n",
    "            x (torch.tensor): the initial sample\n",
    "            t (torch.tensor) time points where we suppose x is from t[0]\n",
    "            return the last sample or the whole seq.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, method='euler', noise_type='diagonal', sde_type='ito', \n",
    "    in_features=2, out_features=2, gunc=None, dt=0.1):\n",
    "        super(ToySDEModel, self).__init__()        \n",
    "        self.func = func\n",
    "        self.method = method\n",
    "        self.noise_type = noise_type\n",
    "        self.sde_type = sde_type\n",
    "        if gunc is None:\n",
    "            self._gunc_args = 'y'\n",
    "            self.gunc = nn.Linear(in_features, out_features)\n",
    "        else:\n",
    "            self._gunc_args = 't,y'\n",
    "            self.gunc = gunc\n",
    "\n",
    "        self.dt = dt\n",
    "        \n",
    "    def f(self, t, y):\n",
    "        return self.func(t, y)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        return self.gunc(t, y) if self._gunc_args == 't,y' else self.gunc(y)\n",
    "        return 0.3 * torch.sigmoid(torch.cos(t) * torch.exp(-y))\n",
    "\n",
    "    def forward(self, x, t, return_whole_sequence=False, dt=None):\n",
    "        dt = self.dt if self.dt is not None else 0.1 if dt is None else dt        \n",
    "        x = torchsde.sdeint(self, x, t, method=self.method, dt=dt)\n",
    "       \n",
    "        x = x[-1] if not return_whole_sequence else x\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
