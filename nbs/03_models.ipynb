{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: models.html\n",
    "title: Models\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "from torch.nn  import functional as F \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class ToyODE(nn.Module):\n",
    "    \"\"\" \n",
    "    ODE derivative network\n",
    "    \n",
    "    feature_dims (int) default '5': dimension of the inputs, either in ambient space or embedded space.\n",
    "    layer (list of int) defaulf ''[64]'': the hidden layers of the network.\n",
    "    activation (torch.nn) default '\"ReLU\"': activation function applied in between layers.\n",
    "    scales (NoneType|list of float) default 'None': the initial scale for the noise in the trajectories. One scale per bin, add more if using an adaptative ODE solver.\n",
    "    n_aug (int) default '1': number of added dimensions to the input of the network. Total dimensions are features_dim + 1 (time) + n_aug. \n",
    "    \n",
    "    Method\n",
    "    forward (Callable)\n",
    "        forward pass of the ODE derivative network.\n",
    "        Parameters:\n",
    "        t (torch.tensor): time of the evaluation.\n",
    "        x (torch.tensor): position of the evalutation.\n",
    "        Return:\n",
    "        derivative at time t and position x.   \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_dims=5,\n",
    "        layers=[64],\n",
    "        activation='ReLU',\n",
    "        scales=None,\n",
    "        n_aug=2\n",
    "    ):\n",
    "        super(ToyODE, self).__init__()\n",
    "        steps = [feature_dims+1+n_aug, *layers, feature_dims]\n",
    "        pairs = zip(steps, steps[1:])\n",
    "\n",
    "        chain = list(itertools.chain(*list(zip(\n",
    "            map(lambda e: nn.Linear(*e), pairs), \n",
    "            itertools.repeat(getattr(nn, activation)())\n",
    "        ))))[:-1]\n",
    "\n",
    "        self.chain = chain\n",
    "        self.seq = (nn.Sequential(*chain))\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.tensor(scales, requires_grad=True).float()) if scales is not None else None\n",
    "        self.n_aug = n_aug        \n",
    "        \n",
    "    def forward(self, t, x): #NOTE the forward pass when we use torchdiffeq must be forward(self,t,x)\n",
    "        zero = torch.tensor([0]).cuda() if x.is_cuda else torch.tensor([0])\n",
    "        zeros = zero.repeat(x.size()[0],self.n_aug)\n",
    "        time = t.repeat(x.size()[0],1)\n",
    "        aug = torch.cat((x,time,zeros),dim=1)\n",
    "        x = self.seq(aug)\n",
    "        if self.alpha is not None:\n",
    "            z = torch.randn(x.size(),requires_grad=False).cuda() if x.is_cuda else torch.randn(x.size(),requires_grad=False)\n",
    "        dxdt = x + z*self.alpha[int(t-1)] if self.alpha is not None else x\n",
    "        return dxdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "from torch.nn  import functional as F \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import os, math, numpy as np\n",
    "\n",
    "\"\"\"\n",
    "WARNING: Only supports local training!\n",
    "\"\"\"\n",
    "class ConditionalODE(nn.Module):\n",
    "    \"\"\" \n",
    "    ODE derivative network\n",
    "    \n",
    "    feature_dims (int) default '5': dimension of the inputs, either in ambient space or embedded space.\n",
    "    layer (list of int) defaulf ''[64]'': the hidden layers of the network.\n",
    "    activation (torch.nn) default '\"ReLU\"': activation function applied in between layers.\n",
    "    scales (NoneType|list of float) default 'None': the initial scale for the noise in the trajectories. One scale per bin, add more if using an adaptative ODE solver.\n",
    "    n_aug (int) default '1': number of added dimensions to the input of the network. Total dimensions are features_dim + 1 (time) + n_aug. \n",
    "    \n",
    "    Method\n",
    "    forward (Callable)\n",
    "        forward pass of the ODE derivative network.\n",
    "        Parameters:\n",
    "        t (torch.tensor): time of the evaluation.\n",
    "        x (torch.tensor): position of the evalutation.\n",
    "        Return:\n",
    "        derivative at time t and position x.   \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_dims=5,\n",
    "        condition_dims=2,\n",
    "        layers=[64],\n",
    "        activation='ReLU',\n",
    "        scales=None,\n",
    "        n_aug=2,\n",
    "        time_homogeneous=True,\n",
    "    ):\n",
    "        super(ConditionalODE, self).__init__()\n",
    "        self.time_homogeneous = time_homogeneous\n",
    "        self.time_dim = 1 if 0 else 1\n",
    "        self.condition_dim = condition_dims\n",
    "        steps = [feature_dims+self.condition_dim+self.time_dim+n_aug, *layers, feature_dims]\n",
    "        pairs = zip(steps, steps[1:])\n",
    "\n",
    "        chain = list(itertools.chain(*list(zip(\n",
    "            map(lambda e: nn.Linear(*e), pairs), \n",
    "            itertools.repeat(getattr(nn, activation)())\n",
    "        ))))[:-1]\n",
    "\n",
    "        self.chain = chain\n",
    "        self.seq = (nn.Sequential(*chain))\n",
    "        \n",
    "        self.alpha = nn.Parameter(torch.tensor(scales, requires_grad=True).float()) if scales is not None else None\n",
    "        self.n_aug = n_aug        \n",
    "        \n",
    "    def set_condition(self, condition):\n",
    "        self.condition = condition\n",
    "\n",
    "    def forward(self, t, x): #NOTE the forward pass when we use torchdiffeq must be forward(self,t,x)\n",
    "        zero = torch.tensor([0]).cuda() if x.is_cuda else torch.tensor([0])\n",
    "        zeros = zero.repeat(x.size()[0],self.n_aug)\n",
    "        time = t.repeat(x.size()[0],self.time_dim)\n",
    "        aug = torch.cat((x,self.condition,time,zeros),dim=1)\n",
    "        x = self.seq(aug)\n",
    "        if self.alpha is not None:\n",
    "            z = torch.randn(x.size(),requires_grad=False).cuda() if x.is_cuda else torch.randn(x.size(),requires_grad=False)\n",
    "        dxdt = x + z*self.alpha[int(t-1)] if self.alpha is not None else x\n",
    "        return dxdt\n",
    "    \n",
    "\n",
    "\n",
    "class ConditionalModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural ODE\n",
    "        func (nn.Module): The network modeling the derivative.\n",
    "        method (str) defaulf '\"rk4\"': any methods from torchdiffeq.\n",
    "        rtol (NoneType | float): the relative tolerance of the ODE solver.\n",
    "        atol (NoneType | float): the absolute tolerance. of the ODE solver.\n",
    "        use_norm (bool): if True keeps the norm of func.\n",
    "        norm (list of torch.tensor): the norm of the derivative.\n",
    "        \n",
    "        Method\n",
    "        forward (Callable)\n",
    "            x (torch.tensor): the initial sample\n",
    "            t (torch.tensor) time points where we suppose x is from t[0]\n",
    "            return the last sample or the whole seq.      \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, method='rk4', rtol=None, atol=None, use_norm=False):\n",
    "        super(ConditionalModel, self).__init__()        \n",
    "        self.func = func\n",
    "        self.method = method\n",
    "        self.rtol=rtol\n",
    "        self.atol=atol\n",
    "        self.use_norm = use_norm\n",
    "        self.norm=[]\n",
    "\n",
    "    def forward(self, x, t, return_whole_sequence=False):\n",
    "        # assume the last `condition_dim` dimensions of x is the condition\n",
    "        x = x[:,:-self.func.condition_dim]\n",
    "        c = x[:,-self.func.condition_dim:]\n",
    "        self.func.set_condition(c)\n",
    "        if self.use_norm:\n",
    "            for time in t: \n",
    "                self.norm.append(torch.linalg.norm(self.func(time,x)).pow(2))\n",
    "        if self.atol is None and self.rtol is None:\n",
    "            x = odeint(self.func,x ,t, method=self.method)\n",
    "        elif self.atol is not None and self.rtol is None:\n",
    "            x = odeint(self.func,x ,t, method=self.method, atol=self.atol)\n",
    "        elif self.atol is None and self.rtol is not None:\n",
    "            x = odeint(self.func,x ,t, method=self.method, rtol=self.rtol)\n",
    "        else: \n",
    "            x = odeint(self.func,x ,t, method=self.method, atol=self.atol, rtol=self.rtol)          \n",
    "       \n",
    "        x = x[-1] if not return_whole_sequence else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "from torch.nn  import functional as F \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os, math, numpy as np\n",
    "\n",
    "class GrowthRateModel(nn.Module):\n",
    "    def __init__(self, feature_dims, condition_dims, layers=[64], activation='ReLU', use_time=True):\n",
    "        super(GrowthRateModel, self).__init__()\n",
    "        self.feature_dims = feature_dims\n",
    "        self.condition_dims = condition_dims\n",
    "        self.layers = layers\n",
    "        self.activation = activation\n",
    "        self.use_time = use_time\n",
    "        self.mlp = nn.Sequential()\n",
    "        self.mlp.append(nn.Linear(feature_dims + condition_dims + (1 if use_time else 0), layers[0]))\n",
    "        self.mlp.append(getattr(nn, activation)())\n",
    "        for i in range(len(layers)-1):\n",
    "            self.mlp.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            self.mlp.append(getattr(nn, activation)())\n",
    "        self.mlp.append(nn.Linear(layers[-1], 1)) # output the growth rate\n",
    "\n",
    "    def forward(self, x, c=None, t=None):\n",
    "        if self.condition_dims > 0:\n",
    "            assert c is not None, \"Condition must be provided\"\n",
    "            x = torch.cat([x, c], dim=1)\n",
    "        if self.use_time:\n",
    "            t = t.unsqueeze(1)\n",
    "            x = torch.cat([x, t], dim=1)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_growth_rate_model():\n",
    "    # Test initialization\n",
    "    feature_dims = 3\n",
    "    condition_dims = 2\n",
    "    layers = [64, 32]\n",
    "    model = GrowthRateModel(feature_dims, condition_dims, layers)\n",
    "    \n",
    "    # Test forward pass with condition and time\n",
    "    batch_size = 10\n",
    "    x = torch.randn(batch_size, feature_dims)\n",
    "    c = torch.randn(batch_size, condition_dims) \n",
    "    t = torch.randn(batch_size)\n",
    "    output = model(x, c, t)\n",
    "    assert output.shape == (batch_size, 1), f\"Expected output shape {(batch_size, 1)}, got {output.shape}\"\n",
    "    \n",
    "    # Test forward pass without time\n",
    "    model_no_time = GrowthRateModel(feature_dims, condition_dims, layers, use_time=False)\n",
    "    output_no_time = model_no_time(x, c)\n",
    "    assert output_no_time.shape == (batch_size, 1)\n",
    "    \n",
    "    # Test without conditions\n",
    "    model_no_cond = GrowthRateModel(feature_dims, 0, layers, use_time=True)\n",
    "    output_no_cond = model_no_cond(x, None, t)\n",
    "    assert output_no_cond.shape == (batch_size, 1)\n",
    "    \n",
    "    # Test error cases\n",
    "    try:\n",
    "        # Should fail when condition is required but not provided\n",
    "        model(x, None, t)\n",
    "        assert False, \"Should have raised an AssertionError for missing condition\"\n",
    "    except AssertionError:\n",
    "        pass\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_growth_rate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\"\"\"I want to deprecate this function.\"\"\"\n",
    "def make_model(\n",
    "    feature_dims=5,\n",
    "    layers=[64],\n",
    "    output_dims=5,\n",
    "    activation='ReLU',\n",
    "    which='ode',\n",
    "    method='rk4',\n",
    "    rtol=None,\n",
    "    atol=None,\n",
    "    scales=None,\n",
    "    n_aug=2,\n",
    "    noise_type='diagonal', sde_type='ito',\n",
    "    use_norm=False,\n",
    "    use_cuda=False,\n",
    "    in_features=2, out_features=2, gunc=None,\n",
    "    n_conditions=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates the 'ode' model or 'sde' model or the Geodesic Autoencoder. \n",
    "    See the parameters of the respective classes.\n",
    "    \"\"\"\n",
    "    if which == 'ode' and n_conditions == 0:\n",
    "        ode = ToyODE(feature_dims, layers, activation,scales,n_aug)\n",
    "        model = ToyModel(ode,method,rtol, atol, use_norm=use_norm)\n",
    "    elif which == 'ode' and n_conditions > 0:\n",
    "        ode = ConditionalODE(feature_dims, n_conditions, layers, activation, scales, n_aug)\n",
    "        model = ConditionalModel(ode, method, rtol, atol, use_norm=use_norm)\n",
    "    elif which == 'sde':\n",
    "        ode = ToyODE(feature_dims, layers, activation,scales,n_aug)\n",
    "        model = ToySDEModel(\n",
    "            ode, method, noise_type, sde_type,\n",
    "            in_features=in_features, out_features=out_features, gunc=gunc\n",
    "            \n",
    "        )\n",
    "    else:\n",
    "        model = ToyGeo(feature_dims, layers, output_dims, activation)\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "from torch.nn  import functional as F \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Geodesic Autoencoder\n",
    "    \n",
    "    encoder_layers (list of int) default '[100, 100, 20]': encoder_layers[0] is the feature dimension, and encoder_layers[-1] the embedded dimension.\n",
    "    decoder_layers (list of int) defaulf '[20, 100, 100]': decoder_layers[0] is the embbeded dim and decoder_layers[-1] the feature dim.\n",
    "    activation (torch.nn) default '\"Tanh\"': activation function applied in between layers.\n",
    "    use_cuda (bool) default to False: Whether to use GPU or CPU.\n",
    "    \n",
    "    Method\n",
    "    encode\n",
    "        forward pass of the encoder\n",
    "        x (torch.tensor): observations\n",
    "        Return:\n",
    "        the encoded observations\n",
    "    decode\n",
    "        forward pass of the decoder\n",
    "        z (torch.tensor): embedded observations\n",
    "        Return:\n",
    "        the decoded observations\n",
    "    forward (Callable):\n",
    "        full forward pass, encoder and decoder\n",
    "        x (torch.tensor): observations\n",
    "        Return:\n",
    "        denoised observations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_layers = [100, 100, 20],\n",
    "        decoder_layers = [20, 100, 100],\n",
    "        activation = 'Tanh',\n",
    "        use_cuda = False\n",
    "    ):        \n",
    "        super(Autoencoder, self).__init__()\n",
    "        if decoder_layers is None:\n",
    "            decoder_layers = [*encoder_layers[::-1]]\n",
    "        device = 'cuda' if use_cuda else 'cpu'\n",
    "        \n",
    "        encoder_shapes = list(zip(encoder_layers, encoder_layers[1:]))\n",
    "        decoder_shapes = list(zip(decoder_layers, decoder_layers[1:]))\n",
    "        \n",
    "        encoder_linear = list(map(lambda a: nn.Linear(*a), encoder_shapes))\n",
    "        decoder_linear = list(map(lambda a: nn.Linear(*a), decoder_shapes))\n",
    "        \n",
    "        encoder_riffle = list(itertools.chain(*zip(encoder_linear, itertools.repeat(getattr(nn, activation)()))))[:-1]\n",
    "        encoder = nn.Sequential(*encoder_riffle).to(device)\n",
    "        \n",
    "        decoder_riffle = list(itertools.chain(*zip(decoder_linear, itertools.repeat(getattr(nn, activation)()))))[:-1]\n",
    "\n",
    "        decoder = nn.Sequential(*decoder_riffle).to(device)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        \n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class ToyModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural ODE\n",
    "        func (nn.Module): The network modeling the derivative.\n",
    "        method (str) defaulf '\"rk4\"': any methods from torchdiffeq.\n",
    "        rtol (NoneType | float): the relative tolerance of the ODE solver.\n",
    "        atol (NoneType | float): the absolute tolerance. of the ODE solver.\n",
    "        use_norm (bool): if True keeps the norm of func.\n",
    "        norm (list of torch.tensor): the norm of the derivative.\n",
    "        \n",
    "        Method\n",
    "        forward (Callable)\n",
    "            x (torch.tensor): the initial sample\n",
    "            t (torch.tensor) time points where we suppose x is from t[0]\n",
    "            return the last sample or the whole seq.      \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, method='rk4', rtol=None, atol=None, use_norm=False):\n",
    "        super(ToyModel, self).__init__()        \n",
    "        self.func = func\n",
    "        self.method = method\n",
    "        self.rtol=rtol\n",
    "        self.atol=atol\n",
    "        self.use_norm = use_norm\n",
    "        self.norm=[]\n",
    "\n",
    "    def forward(self, x, t, return_whole_sequence=False):\n",
    "\n",
    "        if self.use_norm:\n",
    "            for time in t: \n",
    "                self.norm.append(torch.linalg.norm(self.func(time,x)).pow(2))\n",
    "        if self.atol is None and self.rtol is None:\n",
    "            x = odeint(self.func,x ,t, method=self.method)\n",
    "        elif self.atol is not None and self.rtol is None:\n",
    "            x = odeint(self.func,x ,t, method=self.method, atol=self.atol)\n",
    "        elif self.atol is None and self.rtol is not None:\n",
    "            x = odeint(self.func,x ,t, method=self.method, rtol=self.rtol)\n",
    "        else: \n",
    "            x = odeint(self.func,x ,t, method=self.method, atol=self.atol, rtol=self.rtol)          \n",
    "       \n",
    "        x = x[-1] if not return_whole_sequence else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from torchdiffeq import odeint_adjoint as odeint\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsde\n",
    "\n",
    "class ToySDEModel(nn.Module):\n",
    "    \"\"\" \n",
    "    Neural SDE model\n",
    "        func (nn.Module): drift term.\n",
    "        genc (nn.Module): diffusion term.\n",
    "        method (str): method of the SDE solver.\n",
    "        \n",
    "        Method\n",
    "        forward (Callable)\n",
    "            x (torch.tensor): the initial sample\n",
    "            t (torch.tensor) time points where we suppose x is from t[0]\n",
    "            return the last sample or the whole seq.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, func, method='euler', noise_type='diagonal', sde_type='ito', \n",
    "    in_features=2, out_features=2, gunc=None, dt=0.1):\n",
    "        super(ToySDEModel, self).__init__()        \n",
    "        self.func = func\n",
    "        self.method = method\n",
    "        self.noise_type = noise_type\n",
    "        self.sde_type = sde_type\n",
    "        if gunc is None:\n",
    "            self._gunc_args = 'y'\n",
    "            self.gunc = nn.Linear(in_features, out_features)\n",
    "        else:\n",
    "            self._gunc_args = 't,y'\n",
    "            self.gunc = gunc\n",
    "\n",
    "        self.dt = dt\n",
    "        \n",
    "    def f(self, t, y):\n",
    "        return self.func(t, y)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        return self.gunc(t, y) if self._gunc_args == 't,y' else self.gunc(y)\n",
    "        return 0.3 * torch.sigmoid(torch.cos(t) * torch.exp(-y))\n",
    "\n",
    "    def forward(self, x, t, return_whole_sequence=False, dt=None):\n",
    "        dt = self.dt if self.dt is not None else 0.1 if dt is None else dt        \n",
    "        x = torchsde.sdeint(self, x, t, method=self.method, dt=dt)\n",
    "       \n",
    "        x = x[-1] if not return_whole_sequence else x\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
