{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: train.html\n",
    "title: Train\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp train\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, sys, json, math, itertools\n",
    "import pandas as pd, numpy as np\n",
    "import warnings\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from MIOFlow.utils import sample, generate_steps\n",
    "from MIOFlow.losses import MMD_loss, OT_loss, Density_loss, Local_density_loss, density_specified_OT_loss, EnergyLoss, EnergyLossGrowthRate, EnergyLossSeq, EnergyLossGrowthRateSeq\n",
    "from MIOFlow.models import GrowthRateModel\n",
    "\n",
    "def train(\n",
    "    model, df, groups, optimizer, n_batches=20, \n",
    "    criterion=MMD_loss(),\n",
    "    use_cuda=False,\n",
    "\n",
    "    sample_size=(100, ),\n",
    "    sample_with_replacement=False,\n",
    "\n",
    "    local_loss=True,\n",
    "    global_loss=False,\n",
    "\n",
    "    hold_one_out=False,\n",
    "    hold_out='random',\n",
    "    apply_losses_in_time=True,\n",
    "\n",
    "    top_k = 5,\n",
    "    hinge_value = 0.01,\n",
    "    use_density_loss=True,\n",
    "    density_detach_m = True,\n",
    "    # use_local_density=False,\n",
    "\n",
    "    lambda_density = 1.0,\n",
    "\n",
    "    autoencoder=None, \n",
    "    use_emb=True,\n",
    "    use_gae=False,\n",
    "\n",
    "    use_gaussian:bool=True, \n",
    "    add_noise:bool=False, \n",
    "    noise_scale:float=0.1,\n",
    "    \n",
    "    logger=None,\n",
    "\n",
    "    use_penalty=False,\n",
    "    lambda_energy=1.0,\n",
    "\n",
    "    reverse:bool = False,\n",
    "    lambda_m = 0.,\n",
    "\n",
    "    use_penalty_m = False,\n",
    "    lambda_energy_m = 1.0,\n",
    "    lambda_ot = 1.0,\n",
    "    energy_weighted=True,\n",
    "    energy_detach_m=False,\n",
    "):\n",
    "\n",
    "    '''\n",
    "    MIOFlow training loop\n",
    "    \n",
    "    Notes:\n",
    "        - The argument `model` must have a method `forward` that accepts two arguments\n",
    "            in its function signature:\n",
    "                ```python\n",
    "                model.forward(x, t)\n",
    "                ```\n",
    "            where, `x` is the input tensor and `t` is a `torch.Tensor` of time points (float).\n",
    "        - The training loop is divided in two parts; local (predict t+1 from t), and global (predict the entire trajectory).\n",
    "                        \n",
    "    Arguments:\n",
    "        model (nn.Module): the initialized pytorch ODE model.\n",
    "        \n",
    "        df (pd.DataFrame): the DataFrame from which to extract batch data.\n",
    "        \n",
    "        groups (list): the list of the numerical groups in the data, e.g. \n",
    "            `[1.0, 2.0, 3.0, 4.0, 5.0]`, if the data has five groups.\n",
    "    \n",
    "        optimizer (torch.optim): an optimizer initilized with the model's parameters.\n",
    "        \n",
    "        n_batches (int): Default to '20', the number of batches from which to randomly sample each consecutive pair\n",
    "            of groups.\n",
    "            \n",
    "        criterion (Callable | nn.Loss): a loss function.\n",
    "        \n",
    "        use_cuda (bool): Defaults to `False`. Whether or not to send the model and data to cuda. \n",
    "\n",
    "        sample_size (tuple): Defaults to `(100, )`\n",
    "\n",
    "        sample_with_replacement (bool): Defaults to `False`. Whether or not to sample data points with replacement.\n",
    "        \n",
    "        local_loss (bool): Defaults to `True`. Whether or not to use a local loss in the model.\n",
    "            See notes for more detail.\n",
    "            \n",
    "        global_loss (bool): Defaults to `False`. Whether or not to use a global loss in the model.\n",
    "        \n",
    "        hold_one_out (bool): Defaults to `False`. Whether or not to randomly hold one time pair\n",
    "            e.g. t_1 to t_2 out when computing the global loss.\n",
    "\n",
    "        hold_out (str | int): Defaults to `\"random\"`. Which time point to hold out when calculating the\n",
    "            global loss.\n",
    "            \n",
    "        apply_losses_in_time (bool): Defaults to `True`. Applies the losses and does back propegation\n",
    "            as soon as a loss is calculated. See notes for more detail.\n",
    "\n",
    "        top_k (int): Default to '5'. The k for the k-NN used in the density loss.\n",
    "\n",
    "        hinge_value (float): Defaults to `0.01`. The hinge value for density loss.\n",
    "\n",
    "        use_density_loss (bool): Defaults to `True`. Whether or not to add density regularization.\n",
    "\n",
    "        lambda_density (float): Defaults to `1.0`. The weight for density loss.\n",
    "\n",
    "        autoencoder (NoneType|nn.Module): Default to 'None'. The full geodesic Autoencoder.\n",
    "\n",
    "        use_emb (bool): Defaults to `True`. Whether or not to use the embedding model.\n",
    "        \n",
    "        use_gae (bool): Defaults to `False`. Whether or not to use the full Geodesic AutoEncoder.\n",
    "\n",
    "        use_gaussian (bool): Defaults to `True`. Whether to use random or gaussian noise.\n",
    "\n",
    "        add_noise (bool): Defaults to `False`. Whether or not to add noise.\n",
    "\n",
    "        noise_scale (float): Defaults to `0.30`. How much to scale the noise by.\n",
    "        \n",
    "        logger (NoneType|Logger): Default to 'None'. The logger to record information.\n",
    "\n",
    "        use_penalty (bool): Defaults to `False`. Whether or not to use $L_e$ during training (norm of the derivative).\n",
    "        \n",
    "        lambda_energy (float): Default to '1.0'. The weight of the energy penalty.\n",
    "\n",
    "        reverse (bool): Whether to train time backwards.\n",
    "    '''\n",
    "\n",
    "    \"\"\"\n",
    "    Xingzhi: changed the energy penalty to being computed outside the model.\n",
    "    \"\"\"\n",
    "    if autoencoder is None and (use_emb or use_gae):\n",
    "        use_emb = False\n",
    "        use_gae = False\n",
    "        warnings.warn('\\'autoencoder\\' is \\'None\\', but \\'use_emb\\' or \\'use_gae\\' is True, both will be set to False.')\n",
    "\n",
    "    noise_fn = torch.randn if use_gaussian else torch.rand\n",
    "    def noise(data):\n",
    "        return noise_fn(*data.shape).cuda() if use_cuda else noise_fn(*data.shape)\n",
    "    # Create the indicies for the steps that should be used\n",
    "    steps = generate_steps(groups)\n",
    "\n",
    "    if reverse:\n",
    "        groups = groups[::-1]\n",
    "        steps = generate_steps(groups)\n",
    "\n",
    "    \n",
    "    # Storage variables for losses\n",
    "    batch_losses = []\n",
    "    globe_losses = []\n",
    "    if hold_one_out and hold_out in groups:\n",
    "        groups_ho = [g for g in groups if g != hold_out]\n",
    "        local_losses = {f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups_ho) if hold_out not in [t0, t1]}\n",
    "    else:\n",
    "        local_losses = {f'{t0}:{t1}':[] for (t0, t1) in steps}\n",
    "        \n",
    "    density_fn = Density_loss(hinge_value) # if not use_local_density else Local_density_loss()\n",
    "    # Send model to cuda and specify it as training mode\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    if isinstance(model, GrowthRateModel):\n",
    "        growth_rate = True\n",
    "        assert isinstance(criterion, density_specified_OT_loss), 'Criterion must be density_specified_OT_loss when using growth_rate=True'\n",
    "    else:\n",
    "        growth_rate = False\n",
    "\n",
    "    # if use_penalty:\n",
    "    #     model.use_norm = True\n",
    "    \n",
    "    # if use_penalty_m:\n",
    "    #     model.use_norm_m = True\n",
    "\n",
    "    energy_loss_growth_rate = EnergyLossGrowthRate(weighted=energy_weighted, detach_m=energy_detach_m)\n",
    "    energy_loss_growth_rate_seq = EnergyLossGrowthRateSeq(weighted=energy_weighted, detach_m=energy_detach_m)\n",
    "    energy_loss = EnergyLoss()\n",
    "    energy_loss_seq = EnergyLossSeq()\n",
    "\n",
    "    assert use_penalty == False and use_penalty_m == False, 'Use energy penalty instead of norm penalty!'\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch in tqdm(range(n_batches)):\n",
    "        \n",
    "        # apply local loss\n",
    "        if local_loss and not global_loss:\n",
    "            # for storing the local loss with calling `.item()` so `loss.backward()` can still be used\n",
    "            batch_loss = []\n",
    "            if hold_one_out:\n",
    "                groups = [g for g in groups if g != hold_out] # TODO: Currently does not work if hold_out='random'. Do to_ignore before. \n",
    "                steps = generate_steps(groups)\n",
    "            for step_idx, (t0, t1) in enumerate(steps):  \n",
    "                if hold_out in [t0, t1] and hold_one_out: # TODO: This `if` can be deleted since the groups does not include the ho timepoint anymore\n",
    "                    continue                              # i.e. it is always False. \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #sampling, predicting, and evaluating the loss.\n",
    "                # sample data\n",
    "                data_t0 = sample(df, t0, size=sample_size, replace=sample_with_replacement, to_torch=True, use_cuda=use_cuda)\n",
    "                data_t1 = sample(df, t1, size=sample_size, replace=sample_with_replacement, to_torch=True, use_cuda=use_cuda)\n",
    "                time = torch.Tensor([t0, t1]).cuda() if use_cuda else torch.Tensor([t0, t1])\n",
    "\n",
    "                if add_noise:\n",
    "                    data_t0 += noise(data_t0) * noise_scale\n",
    "                    data_t1 += noise(data_t1) * noise_scale\n",
    "                if autoencoder is not None and use_gae:\n",
    "                    data_t0 = autoencoder.encoder(data_t0)\n",
    "                    data_t1 = autoencoder.encoder(data_t1)\n",
    "                # prediction\n",
    "                if growth_rate:\n",
    "                    data_tp, m_tp = model(data_t0, time)\n",
    "                else:\n",
    "                    data_tp = model(data_t0, time)\n",
    "\n",
    "                if autoencoder is not None and use_emb:        \n",
    "                    data_tp, data_t1 = autoencoder.encoder(data_tp), autoencoder.encoder(data_t1)\n",
    "                # loss between prediction and sample t1\n",
    "                if growth_rate:\n",
    "                    loss = lambda_ot * criterion(data_tp, data_t1, m_tp)\n",
    "                else:\n",
    "                    loss = lambda_ot * criterion(data_tp, data_t1)\n",
    "\n",
    "                if use_density_loss: \n",
    "                    if growth_rate:\n",
    "                        if density_detach_m:\n",
    "                            density_loss = density_fn(data_tp, data_t1, pre_softmax_weights=m_tp.detach(), top_k=top_k)\n",
    "                        else:\n",
    "                            density_loss = density_fn(data_tp, data_t1, pre_softmax_weights=m_tp, top_k=top_k)\n",
    "                    else:               \n",
    "                        density_loss = density_fn(data_tp, data_t1, top_k=top_k)\n",
    "                    density_loss = density_loss.to(loss.device)\n",
    "                    loss += lambda_density * density_loss\n",
    "\n",
    "                # if use_penalty:\n",
    "                #     if growth_rate:\n",
    "                #         NotImplemented\n",
    "                #     else:\n",
    "                #         dxdx = model.func(time, data_tp)\n",
    "                #     penalty = sum(model.norm)\n",
    "                #     loss += lambda_energy * penalty\n",
    "                \n",
    "                # if use_penalty_m:\n",
    "                #     penalty_m = sum(model.norm_m)\n",
    "                #     loss += lambda_energy_m * penalty_m\n",
    "                \n",
    "                if growth_rate and (lambda_energy > 0 or lambda_energy_m > 0):\n",
    "                    eloss, emloss = energy_loss_growth_rate(model, data_tp, m_tp, time[-1])\n",
    "                    loss += lambda_energy * eloss\n",
    "                    loss += lambda_energy_m * emloss\n",
    "                elif lambda_energy > 0:\n",
    "                    eloss = energy_loss(model, data_tp, time[-1])\n",
    "                    loss += lambda_energy * eloss\n",
    "\n",
    "                if growth_rate and lambda_m > 0:\n",
    "                    # now taking the mean over all points, \n",
    "                    # because we allow individual points to be large or small in mass,\n",
    "                    # but we want the average to stay the same for stablity.\n",
    "                    m_loss = (torch.square(m_tp.mean(axis=-1) - model.m_init)).mean() \n",
    "                    loss += lambda_m * m_loss\n",
    "\n",
    "                # apply local loss as we calculate it\n",
    "                if apply_losses_in_time and local_loss:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # model.norm=[]\n",
    "                    # model.norm_m=[]\n",
    "                # save loss in storage variables \n",
    "                local_losses[f'{t0}:{t1}'].append(loss.item())\n",
    "                batch_loss.append(loss)\n",
    "        \n",
    "        \n",
    "            # convert the local losses into a tensor of len(steps)\n",
    "            batch_loss = torch.Tensor(batch_loss).float()\n",
    "            if use_cuda:\n",
    "                batch_loss = batch_loss.cuda()\n",
    "            \n",
    "            if not apply_losses_in_time:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # store average / sum of local losses for training\n",
    "            ave_local_loss = torch.mean(batch_loss)\n",
    "            sum_local_loss = torch.sum(batch_loss)            \n",
    "            batch_losses.append(ave_local_loss.item())\n",
    "        \n",
    "        # apply global loss\n",
    "        elif global_loss and not local_loss:\n",
    "            optimizer.zero_grad()\n",
    "            #sampling, predicting, and evaluating the loss.\n",
    "            # sample data\n",
    "            data_ti = [\n",
    "                sample(\n",
    "                    df, group, size=sample_size, replace=sample_with_replacement, \n",
    "                    to_torch=True, use_cuda=use_cuda\n",
    "                )\n",
    "                for group in groups\n",
    "            ]\n",
    "            time = torch.Tensor(groups).cuda() if use_cuda else torch.Tensor(groups)\n",
    "\n",
    "            if add_noise:\n",
    "                data_ti = [\n",
    "                    data + noise(data) * noise_scale for data in data_ti\n",
    "                ]\n",
    "            if autoencoder is not None and use_gae:\n",
    "                data_ti = [autoencoder.encoder(data) for data in data_ti]\n",
    "            # prediction\n",
    "            if growth_rate:\n",
    "                data_tp, m_tp = model(data_ti[0], time, return_whole_sequence=True)\n",
    "            else:\n",
    "                data_tp = model(data_ti[0], time, return_whole_sequence=True)\n",
    "            if autoencoder is not None and use_emb:        \n",
    "                data_tp = [autoencoder.encoder(data) for data in data_tp]\n",
    "                data_ti = [autoencoder.encoder(data) for data in data_ti]\n",
    "\n",
    "            #ignoring one time point\n",
    "            to_ignore = None #TODO: This assignment of `to_ingnore`, could be moved at the beginning of the function. \n",
    "            if hold_one_out and hold_out == 'random':\n",
    "                to_ignore = np.random.choice(groups)\n",
    "            elif hold_one_out and hold_out in groups:\n",
    "                to_ignore = hold_out\n",
    "            elif hold_one_out:\n",
    "                raise ValueError('Unknown group to hold out')\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            non_ignore_idx = [i for i in range(len(groups)) if groups[i] != to_ignore]\n",
    "\n",
    "            # print(\"non_ignore_idx\", non_ignore_idx)\n",
    "\n",
    "            if growth_rate:\n",
    "                loss = sum([\n",
    "                    criterion(data_tp[i], data_ti[i], m_tp[i]) \n",
    "                    for i in range(1, len(groups))\n",
    "                    if groups[i] != to_ignore\n",
    "                ])\n",
    "                loss = lambda_ot * loss\n",
    "            else:\n",
    "                loss = sum([\n",
    "                    criterion(data_tp[i], data_ti[i]) \n",
    "                    for i in range(1, len(groups))\n",
    "                    if groups[i] != to_ignore\n",
    "                ])\n",
    "                loss = lambda_ot * loss\n",
    "\n",
    "            if use_density_loss:                \n",
    "                if growth_rate:\n",
    "                    if density_detach_m:\n",
    "                        density_loss = density_fn(data_tp, data_ti, groups, to_ignore, top_k, pre_softmax_weights=m_tp.detach())\n",
    "                    else:\n",
    "                        density_loss = density_fn(data_tp, data_ti, groups, to_ignore, top_k, pre_softmax_weights=m_tp)\n",
    "                else:               \n",
    "                    density_loss = density_fn(data_tp, data_ti, groups, to_ignore, top_k)\n",
    "                density_loss = density_loss.to(loss.device)\n",
    "                loss += lambda_density * density_loss\n",
    "\n",
    "            if growth_rate and (lambda_energy > 0 or lambda_energy_m > 0):\n",
    "                eloss, emloss = energy_loss_growth_rate_seq(model, data_tp[non_ignore_idx,...], m_tp[non_ignore_idx,...], time[non_ignore_idx,...])\n",
    "                loss += lambda_energy * eloss\n",
    "                loss += lambda_energy_m * emloss\n",
    "            elif lambda_energy > 0:\n",
    "                eloss = energy_loss_seq(model, data_tp[non_ignore_idx,...], time[non_ignore_idx,...])\n",
    "                loss += lambda_energy * eloss\n",
    "\n",
    "            # if use_penalty:\n",
    "            #     penalty = sum([model.norm[-(i+1)] for i in range(1, len(groups))\n",
    "            #         if groups[i] != to_ignore])\n",
    "            #     loss += lambda_energy * penalty\n",
    "\n",
    "            # if use_penalty_m:\n",
    "            #     penalty_m = sum([model.norm_m[-(i+1)] for i in range(1, len(groups))\n",
    "            #         if groups[i] != to_ignore])\n",
    "            #     loss += lambda_energy_m * penalty_m\n",
    "\n",
    "            if growth_rate and lambda_m > 0:\n",
    "                # now taking the mean over all points, \n",
    "                # because we allow individual points to be large or small in mass,\n",
    "                # but we want the average to stay the same for stablity.\n",
    "                m_loss = (torch.square(m_tp[non_ignore_idx,...].mean(axis=-1) - model.m_init)).mean() \n",
    "                loss += lambda_m * m_loss\n",
    "                                       \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.norm=[]\n",
    "            model.norm_m=[]\n",
    "\n",
    "            globe_losses.append(loss.item())\n",
    "        elif local_loss and global_loss:\n",
    "            # NOTE: weighted local / global loss has been removed to improve runtime\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            raise ValueError('A form of loss must be specified.')\n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(f\"NaN loss encountered at batch {batch}. Stopping training.\")\n",
    "             \n",
    "    print_loss = globe_losses if global_loss else batch_losses \n",
    "    if logger is None:      \n",
    "        tqdm.write(f'Train loss: {np.round(np.mean(print_loss), 5)}')\n",
    "    else:\n",
    "        logger.info(f'Train loss: {np.round(np.mean(print_loss), 5)}')\n",
    "    return local_losses, batch_losses, globe_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from MIOFlow.utils import generate_steps\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train_ae(\n",
    "    model, df, groups, optimizer,\n",
    "    n_epochs=60, criterion=nn.MSELoss(), dist=None, recon = True,\n",
    "    use_cuda=False, sample_size=(100, ),\n",
    "    sample_with_replacement=False,\n",
    "    noise_min_scale=0.09,\n",
    "    noise_max_scale=0.15,\n",
    "    hold_one_out:bool=False,\n",
    "    hold_out='random'\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    Geodesic Autoencoder training loop.\n",
    "    \n",
    "    Notes:\n",
    "        - We can train only the encoder the fit the geodesic distance (recon=False), or the full geodesic Autoencoder (recon=True),\n",
    "            i.e. matching the distance and reconstruction of the inputs.\n",
    "            \n",
    "    Arguments:\n",
    "    \n",
    "        model (nn.Module): the initialized pytorch Geodesic Autoencoder model.\n",
    "\n",
    "        df (pd.DataFrame): the DataFrame from which to extract batch data.\n",
    "        \n",
    "        groups (list): the list of the numerical groups in the data, e.g. \n",
    "            `[1.0, 2.0, 3.0, 4.0, 5.0]`, if the data has five groups.\n",
    "\n",
    "        optimizer (torch.optim): an optimizer initilized with the model's parameters.\n",
    "\n",
    "        n_epochs (int): Default to '60'. The number of training epochs.\n",
    "\n",
    "        criterion (torch.nn). Default to 'nn.MSELoss()'. The criterion to minimize. \n",
    "\n",
    "        dist (NoneType|Class). Default to 'None'. The distance Class with a 'fit(X)' method for a dataset 'X'. Computes the pairwise distances in 'X'.\n",
    "\n",
    "        recon (bool): Default to 'True'. Whether or not the apply the reconstruction loss. \n",
    "        \n",
    "        use_cuda (bool): Defaults to `False`. Whether or not to send the model and data to cuda. \n",
    "        \n",
    "        sample_size (tuple): Defaults to `(100, )`.\n",
    "        \n",
    "        sample_with_replacement (bool): Defaults to `False`. Whether or not to sample data points with replacement.\n",
    "        \n",
    "        noise_min_scale (float): Default to '0.0'. The minimum noise scale. \n",
    "        \n",
    "        noise_max_scale (float): Default to '1.0'. The maximum noise scale. The true scale is sampled between these two bounds for each epoch. \n",
    "        \n",
    "        hold_one_out (bool): Default to False, whether or not to ignore a timepoint during training.\n",
    "        \n",
    "        hold_out (str|int): Default to 'random', the timepoint to hold out, either a specific element of 'groups' or a random one. \n",
    "    \n",
    "    \"\"\"\n",
    "    steps = generate_steps(groups)\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        \n",
    "        # ignoring one time point\n",
    "        to_ignore = None\n",
    "        if hold_one_out and hold_out == 'random':\n",
    "            to_ignore = np.random.choice(groups)\n",
    "        elif hold_one_out and hold_out in groups:\n",
    "            to_ignore = hold_out\n",
    "        elif hold_one_out:\n",
    "            raise ValueError('Unknown group to hold out')\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Training\n",
    "        optimizer.zero_grad()\n",
    "        noise_scale = torch.FloatTensor(1).uniform_(noise_min_scale, noise_max_scale)\n",
    "        data_ti = torch.vstack([sample(df, group, size=sample_size, replace=sample_with_replacement, to_torch=True, use_cuda=use_cuda) for group in groups if group != to_ignore])\n",
    "        noise = (noise_scale*torch.randn(data_ti.size())).cuda() if use_cuda else noise_scale*torch.randn(data_ti.size())\n",
    "        \n",
    "        encode_dt = model.encoder(data_ti + noise)\n",
    "        recon_dt = model.decoder(encode_dt) if recon else None\n",
    "        \n",
    "        if recon:\n",
    "            loss_recon = criterion(recon_dt,data_ti)\n",
    "            loss = loss_recon\n",
    "            \n",
    "            if epoch%50==0:\n",
    "                tqdm.write(f'Train loss recon: {np.round(np.mean(loss_recon.item()), 5)}')\n",
    "        \n",
    "        if dist is not None:\n",
    "            dist_geo = dist.fit(data_ti.cpu().numpy())\n",
    "            dist_geo = torch.from_numpy(dist_geo).float().cuda() if use_cuda else torch.from_numpy(dist_geo).float()\n",
    "            dist_emb = torch.cdist(encode_dt,encode_dt)**2\n",
    "            loss_dist = criterion(dist_emb,dist_geo)\n",
    "            loss = loss_recon + loss_dist if recon else loss_dist\n",
    "            \n",
    "            if epoch%50==0:\n",
    "                tqdm.write(f'Train loss dist: {np.round(np.mean(loss_dist.item()), 5)}')\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "from MIOFlow.plots import plot_comparision, plot_losses\n",
    "from MIOFlow.eval import generate_plot_data\n",
    "\n",
    "def training_regimen(\n",
    "    n_local_epochs, n_epochs, n_post_local_epochs,\n",
    "    exp_dir, \n",
    "\n",
    "    # BEGIN: train params\n",
    "    model, df, groups, optimizer, n_batches=20, \n",
    "    criterion=MMD_loss(), use_cuda=False,\n",
    "\n",
    "\n",
    "    hold_one_out=False, hold_out='random', \n",
    "    hinge_value=0.01, use_density_loss=True, density_detach_m=True,\n",
    "\n",
    "    top_k = 5, lambda_density = 1.0, \n",
    "    autoencoder=None, use_emb=True, use_gae=False, \n",
    "    sample_size=(100, ), \n",
    "    sample_with_replacement=False, \n",
    "    logger=None, \n",
    "    add_noise=False, noise_scale=0.1, use_gaussian=True,  \n",
    "    use_penalty=False, lambda_energy=1.0,\n",
    "    # END: train params\n",
    "\n",
    "\n",
    "\n",
    "    steps=None, plot_every=None,\n",
    "    n_points=100, n_trajectories=100, n_bins=100, \n",
    "    local_losses=None, batch_losses=None, globe_losses=None,\n",
    "    reverse_schema=True, reverse_n=4,\n",
    "\n",
    "    # additional train params. appeneded instead of inserted in case some code did not specify parameter names.\n",
    "    lambda_m = 0.,\n",
    "    use_penalty_m = False,\n",
    "    lambda_energy_m = 1.0,\n",
    "    energy_weighted=True,\n",
    "    energy_detach_m=True,\n",
    "    lambda_ot = 1.0\n",
    "):\n",
    "    recon = use_gae and not use_emb\n",
    "    if steps is None:\n",
    "        steps = generate_steps(groups)\n",
    "        \n",
    "    if local_losses is None:\n",
    "        if hold_one_out and hold_out in groups:\n",
    "            groups_ho = [g for g in groups if g != hold_out]\n",
    "            local_losses = {f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups_ho) if hold_out not in [t0, t1]}\n",
    "            if reverse_schema:\n",
    "                local_losses = {\n",
    "                    **local_losses, \n",
    "                    **{f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups_ho[::-1]) if hold_out not in [t0, t1]}\n",
    "                }\n",
    "        else:\n",
    "            local_losses = {f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups)}\n",
    "            if reverse_schema:\n",
    "                local_losses = {\n",
    "                    **local_losses, \n",
    "                    **{f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups[::-1])}\n",
    "                }\n",
    "    if batch_losses is None:\n",
    "        batch_losses = []\n",
    "    if globe_losses is None:\n",
    "        globe_losses = []\n",
    "    \n",
    "    reverse = False\n",
    "    for epoch in tqdm(range(n_local_epochs), desc='Pretraining Epoch'):\n",
    "        reverse = True if reverse_schema and epoch % reverse_n == 0 else False\n",
    "\n",
    "        l_loss, b_loss, g_loss = train(\n",
    "            model, df, groups, optimizer, n_batches, \n",
    "            criterion = criterion, use_cuda = use_cuda,\n",
    "            local_loss=True, global_loss=False, apply_losses_in_time=True,\n",
    "            hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "            hinge_value=hinge_value,\n",
    "            use_density_loss = use_density_loss,    \n",
    "            top_k = top_k, lambda_density = lambda_density, density_detach_m=density_detach_m,\n",
    "            autoencoder = autoencoder, use_emb = use_emb, use_gae = use_gae, sample_size=sample_size, \n",
    "            sample_with_replacement=sample_with_replacement, logger=logger,\n",
    "            add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian, \n",
    "            use_penalty=use_penalty, lambda_energy=lambda_energy, reverse=reverse,\n",
    "            lambda_m=lambda_m,\n",
    "            use_penalty_m=use_penalty_m, lambda_energy_m=lambda_energy_m,\n",
    "            lambda_ot=lambda_ot,\n",
    "            energy_weighted=energy_weighted,\n",
    "            energy_detach_m=energy_detach_m,\n",
    "        )\n",
    "        for k, v in l_loss.items():  \n",
    "            local_losses[k].extend(v)\n",
    "        batch_losses.extend(b_loss)\n",
    "        globe_losses.extend(g_loss)\n",
    "        if plot_every is not None and epoch % plot_every == 0:\n",
    "            generated, trajectories = generate_plot_data(\n",
    "                model, df, n_points, n_trajectories, n_bins, \n",
    "                sample_with_replacement=sample_with_replacement, use_cuda=use_cuda, \n",
    "                samples_key='samples', logger=logger,\n",
    "                autoencoder=autoencoder, recon=recon\n",
    "            )\n",
    "            plot_comparision(\n",
    "                df, generated, trajectories,\n",
    "                palette = 'viridis', df_time_key='samples',\n",
    "                save=True, path=exp_dir, \n",
    "                file=f'2d_comparision_local_{epoch}.png',\n",
    "                x='d1', y='d2', z='d3', is_3d=False\n",
    "            )\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc='Epoch'):\n",
    "        reverse = True if reverse_schema and epoch % reverse_n == 0 else False\n",
    "        l_loss, b_loss, g_loss = train(\n",
    "            model, df, groups, optimizer, n_batches, \n",
    "            criterion = criterion, use_cuda = use_cuda,\n",
    "            local_loss=False, global_loss=True, apply_losses_in_time=True,\n",
    "            hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "            hinge_value=hinge_value,\n",
    "            use_density_loss = use_density_loss,       \n",
    "            top_k = top_k, lambda_density = lambda_density, density_detach_m=density_detach_m,\n",
    "            autoencoder = autoencoder, use_emb = use_emb, use_gae = use_gae, sample_size=sample_size, \n",
    "            sample_with_replacement=sample_with_replacement, logger=logger, \n",
    "            add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian,\n",
    "            use_penalty=use_penalty, lambda_energy=lambda_energy, reverse=reverse,\n",
    "            lambda_m=lambda_m,\n",
    "            use_penalty_m=use_penalty_m, lambda_energy_m=lambda_energy_m,\n",
    "            lambda_ot=lambda_ot,\n",
    "            energy_weighted=energy_weighted,\n",
    "            energy_detach_m=energy_detach_m,\n",
    "        )\n",
    "        for k, v in l_loss.items():  \n",
    "            local_losses[k].extend(v)\n",
    "        batch_losses.extend(b_loss)\n",
    "        globe_losses.extend(g_loss)\n",
    "        if plot_every is not None and epoch % plot_every == 0:\n",
    "            generated, trajectories = generate_plot_data(\n",
    "                model, df, n_points, n_trajectories, n_bins, \n",
    "                sample_with_replacement=sample_with_replacement, use_cuda=use_cuda, \n",
    "                samples_key='samples', logger=logger,\n",
    "                autoencoder=autoencoder, recon=recon\n",
    "            )\n",
    "            plot_comparision(\n",
    "                df, generated, trajectories,\n",
    "                palette = 'viridis', df_time_key='samples',\n",
    "                save=True, path=exp_dir, \n",
    "                file=f'2d_comparision_local_{n_local_epochs}_global_{epoch}.png',\n",
    "                x='d1', y='d2', z='d3', is_3d=False\n",
    "            )\n",
    "        \n",
    "    for epoch in tqdm(range(n_post_local_epochs), desc='Posttraining Epoch'):\n",
    "        reverse = True if reverse_schema and epoch % reverse_n == 0 else False\n",
    "\n",
    "        l_loss, b_loss, g_loss = train(\n",
    "            model, df, groups, optimizer, n_batches, \n",
    "            criterion = criterion, use_cuda = use_cuda,\n",
    "            local_loss=True, global_loss=False, apply_losses_in_time=True,\n",
    "            hold_one_out=hold_one_out, hold_out=hold_out, \n",
    "            hinge_value=hinge_value,\n",
    "            use_density_loss = use_density_loss,       \n",
    "            top_k = top_k, lambda_density = lambda_density, density_detach_m=density_detach_m,\n",
    "            autoencoder = autoencoder, use_emb = use_emb, use_gae = use_gae, sample_size=sample_size, \n",
    "            sample_with_replacement=sample_with_replacement, logger=logger, \n",
    "            add_noise=add_noise, noise_scale=noise_scale, use_gaussian=use_gaussian,\n",
    "            use_penalty=use_penalty, lambda_energy=lambda_energy, reverse=reverse,\n",
    "            lambda_m=lambda_m,\n",
    "            use_penalty_m=use_penalty_m, lambda_energy_m=lambda_energy_m,\n",
    "            lambda_ot=lambda_ot,\n",
    "            energy_weighted=energy_weighted,\n",
    "            energy_detach_m=energy_detach_m,\n",
    "        )\n",
    "        for k, v in l_loss.items():  \n",
    "            local_losses[k].extend(v)\n",
    "        batch_losses.extend(b_loss)\n",
    "        globe_losses.extend(g_loss)\n",
    "        if plot_every is not None and epoch % plot_every == 0:\n",
    "            generated, trajectories = generate_plot_data(\n",
    "                model, df, n_points, n_trajectories, n_bins, \n",
    "                sample_with_replacement=sample_with_replacement, use_cuda=use_cuda, \n",
    "                samples_key='samples', logger=logger,\n",
    "                autoencoder=autoencoder, recon=recon\n",
    "            )\n",
    "            plot_comparision(\n",
    "                df, generated, trajectories,\n",
    "                palette = 'viridis', df_time_key='samples',\n",
    "                save=True, path=exp_dir, \n",
    "                file=f'2d_comparision_local_{n_local_epochs}_global_{n_epochs}_post_{epoch}.png',\n",
    "                x='d1', y='d2', z='d3', is_3d=False\n",
    "            )\n",
    "\n",
    "    if reverse_schema:\n",
    "        _temp = {}\n",
    "        if hold_one_out:\n",
    "            for (t0, t1) in generate_steps([g for g in groups if g != hold_out]):\n",
    "                a = f'{t0}:{t1}'\n",
    "                b = f'{t1}:{t0}'\n",
    "                _temp[a] = []\n",
    "                for i, value in enumerate(local_losses[a]):\n",
    "\n",
    "                    if i % reverse_n == 0:\n",
    "                        _temp[a].append(local_losses[b].pop(0))\n",
    "                        _temp[a].append(value)\n",
    "                    else:\n",
    "                        _temp[a].append(value)\n",
    "            local_losses = _temp\n",
    "        else:\n",
    "            for (t0, t1) in generate_steps(groups):\n",
    "                a = f'{t0}:{t1}'\n",
    "                b = f'{t1}:{t0}'\n",
    "                _temp[a] = []\n",
    "                for i, value in enumerate(local_losses[a]):\n",
    "\n",
    "                    if i % reverse_n == 0:\n",
    "                        _temp[a].append(local_losses[b].pop(0))\n",
    "                        _temp[a].append(value)\n",
    "                    else:\n",
    "                        _temp[a].append(value)\n",
    "            local_losses = _temp\n",
    "\n",
    "\n",
    "\n",
    "    if plot_every is not None:\n",
    "        plot_losses(\n",
    "            local_losses, batch_losses, globe_losses, \n",
    "            save=True, path=exp_dir, \n",
    "            file=f'losses_l{n_local_epochs}_e{n_epochs}_ple{n_post_local_epochs}.png'\n",
    "        )\n",
    "\n",
    "    return local_losses, batch_losses, globe_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
